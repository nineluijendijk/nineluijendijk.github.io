[["index.html", "Curriculum Vitae Contact Education Experience Skills Languages", " Curriculum Vitae Contact LinkedIn: Nine Luijendijk GitHub: nineluijendijk Education Hogeschool Utrecht Bachelor of Science - BS, Life Sciences, Sep. 2020 - Aug. 2024 Specialized in Data Sciences for Biology GPA of 4.0 Radboud University Minor in Adaptive Organisms, Jan. 2023 - Jul. 2023 Experience Netherlands Institute of Ecology (NIOO-KNAW) - Wageningen Intern, Sep. 2023 - present Indebuurt033 - Amersfoort Volunteer, Jan. 2024 - present Zeeman textielSupers - Leusden Sales Associate, Nov. 2021 - Jul. 2023 StudyWorks BV - Leusden Tutor, Jan. 2020 - Sep. 2021 Tutored high school students in English, math and chemistry. Albert Heijn - Leusden Sales Associate, Jan. 2020 - Jul. 2020 Skills R programming language Bash command language SQL Excel CSS stylesheet language (Cell) culture RNA-Seq analysis Languages Dutch - native speaker English - C2 (90 in PTE Academic) "],["guerillaframework.html", "1 Guerilla Analytics framework", " 1 Guerilla Analytics framework To keep my data manageable I use the Guerilla Analytics framework (Ridge 2014). This means I make sure every project has its own folder, every folder that needs it has a README and no raw data file is altered. I also make sure to version control my code using GitHub. The way I manage my data is visible in the tree below: dir_tree(&quot;/Users/nineluijendijk/Desktop/daur2&quot;) knitr::include_graphics(here(&quot;data/screenshot_dirtree.png&quot;)) Figure 1.1: Screenshot of my directory tree References "],["reproducibleresearch.html", "2 Reproducible Research 2.1 Scoring the reproducibility of a research article 2.2 Scoring the reproducibility of the code of the research 2.3 Final rating", " 2 Reproducible Research 2.1 Scoring the reproducibility of a research article 2.1.1 Reproducible research and criteria In this chapter I will be scoring a scientific publication on how reproducible the research is. The criteria for reproducibility can be found in table 2.1. Keeping research reproducible and openly available enables researchers to work together or in parallel towards the same goal. (Sumner et al. 2020) Generating the table dataframe_criteria &lt;- data.frame(TransparencyCriteria = c(&quot;Study Purpose&quot;, &quot;Data Availability Statement&quot;, &quot;Data Location&quot;, &quot;Study Location&quot;, &quot;Author Review&quot;, &quot;Ethics Statement&quot;, &quot;Funding Statement&quot;, &quot;Code Availability&quot;), Definition = c(&quot;A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the studyobjective.&quot;, &quot;A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.&quot;, &quot;Where the article’s data can be accessed, either raw or processed.&quot;, &quot;Author has stated in the methods section where the study took place or the data’s country/region of origin.&quot;, &quot;The professionalism of the contact information that the author has provided in the manuscript.&quot;, &quot;A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.&quot;, &quot;A statement within the manuscript indicating whether or not the authors received funding for their research.&quot;, &quot;Authors have shared access to the most updated code that they used in their study, including code used for analysis.&quot;), ResponseType = c(&quot;Binary&quot;, &quot;Binary&quot;, &quot;Found Value&quot;, &quot;Binary; Found Value&quot;, &quot;Found Value&quot;, &quot;Binary&quot;, &quot;Binary&quot;, &quot;Binary&quot;)) Table containing the criteria of reproducibility knitr::kable(dataframe_criteria, caption = &quot;Table showing the criteria for reproducibilty from @sumnerReproducibilityReportingPractices2020.&quot;) Table 2.1: Table showing the criteria for reproducibilty from Sumner et al. (2020). TransparencyCriteria Definition ResponseType Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the studyobjective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the article’s data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary 2.1.2 The research article The research article I will be scoring for reproducibility is Michalak et al. (2020), “Sounds of sickness: can people identify infectious disease using sounds of coughs and sneezes?”. The study objective is to determine whether humans can identify infectious diseases by hearing. Multiple sound clips, for example of coughing or sneezing, were presented to participants in random order, half of the sounds being from a people with an infectious illness and half from people with a non-infectious condition. The participants were asked to determine the nature of these sounds (infectious or not) and how certain they were. This study was done 3 more times, with slight alterations, for example also asking the participants to score how disgusting a sound was. Using R to calculate statistics, it was found that people can not accurately determine whether a sneeze or cough is infectious, even when they were pretty certain of their judgements. 2.1.3 Reproducibility of the research The reproducibility of this article is scored in table 2.2 below: criteria_scored &lt;- dataframe_criteria %&gt;% mutate(Score = c(&quot;Present&quot;, &quot;Present&quot;, &quot;Present, the data can be found at https://osf.io/4c7vr/.&quot;, &quot;Present, the data was collected from US-based participants.&quot;, &quot;One of four authors&#39; contact information is listed: the address of the university they&#39;re from and their professional email address.&quot;, &quot;Present&quot;, &quot;Present&quot;, &quot;Present&quot;)) knitr::kable(criteria_scored, caption = &quot;Table showing how the article scored on reproducibility.&quot;) Table 2.2: Table showing how the article scored on reproducibility. TransparencyCriteria Definition ResponseType Score Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the studyobjective. Binary Present Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Present Data Location Where the article’s data can be accessed, either raw or processed. Found Value Present, the data can be found at https://osf.io/4c7vr/. Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value Present, the data was collected from US-based participants. Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value One of four authors’ contact information is listed: the address of the university they’re from and their professional email address. Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Present Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Present Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary Present Getting a perfect score, this research seems to be very reproducible. 2.2 Scoring the reproducibility of the code of the research 2.2.1 Criteria for the code Now I will be scoring how easy it is to reproduce one of the figures from the article, using the code provided by the authors. I will also be judging how easy it is to read the code. 2.2.2 The code There is code available that was used to prepare the raw data for further analysis, but the raw data itself is not publically available. I assume this is to protect participants’ identities. The preparation script comes with comments explaining what every step does and is easy to read. Since the researchers mainly use the package {tidyverse} (Hadley Wickham et al. 2019) their code is especially easy to read as I like to use it as well. Using the processed data, it is very easy to recreate the figures from this study. Below is part of the analysis script downloaded from the study’s Open Science Framework (OSF) project (linked in table 2.2). I would rate the readabilty of this code a 5/5. Install and/or load packages library(tidyverse) library(knitr) library(psych) Data #Read data from preparation script qualtrics1 &lt;- read_tsv(&quot;data/study1/cleandata/qualtrics1.tsv&quot;) lqualtrics1 &lt;- read_tsv(&quot;data/study1/cleandata/lqualtrics1.tsv&quot;) Multiple-item Psychological Measures #Perceived Infectability qualtrics1 %&gt;% select(PVD8, PVD12r, PVD2, PVD14r, PVD10, PVD5r, PVD6) %&gt;% omega(title = &quot;Perceived Infectability&quot;) #Scatterplot Matrix (ignoring stimuli variance) lqualtrics1 %&gt;% select(certainty, clarity, pvd_pinfect, pvd_germ, rchild_uncertain, rchild_ses, recentill) %&gt;% pairs.panels(scale = FALSE, pch = &quot;.&quot;) This code didn’t work immediately, there were 2 things I needed to change first. The first thing was the path to the data, since the file names on OSF didn’t perfectly match those in the script. The directory also had to be changed, I used the {here} package Müller (n.d.) so that if I want to refer to the same file from another computer it will still work. The {GPArotation} package (Coen A. Bernaards and Jennrich 2005) also was not loaded in, when it is needed for the omega() function. Besides these 2 things, everything worked fine. The code now looks as follows: Install and/or load packages library(tidyverse) library(knitr) library(psych) library(here) library(GPArotation) Data #Read data from preparation script qualtrics1 &lt;- read_tsv(here(&quot;data_raw/qualtrics1_share.tsv&quot;)) lqualtrics1 &lt;- read_tsv(here(&quot;data_raw/lqualtrics1_share.tsv&quot;)) Multiple-item Psychological Measures #Perceived Infectability qualtrics1 %&gt;% select(PVD8, PVD12r, PVD2, PVD14r, PVD10, PVD5r, PVD6) %&gt;% omega(title = &quot;Perceived Infectability&quot;) Figure 2.1: Plot showing McDonald’s omega estimates (Michalak et al. 2020). Omega estimates results ## Perceived Infectability ## Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, ## digits = digits, title = title, sl = sl, labels = labels, ## plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, ## covar = covar) ## Alpha: 0.91 ## G.6: 0.92 ## Omega Hierarchical: 0.75 ## Omega H asymptotic: 0.79 ## Omega Total 0.94 ## ## Schmid Leiman Factor loadings greater than 0.2 ## g F1* F2* F3* h2 u2 p2 ## PVD8 0.68 0.60 0.83 0.17 0.56 ## PVD12r 0.73 0.34 0.65 0.35 0.81 ## PVD2 0.71 0.37 0.20 0.68 0.32 0.73 ## PVD14r 0.77 0.57 0.91 0.09 0.65 ## PVD10 0.62 0.67 0.83 0.17 0.47 ## PVD5r 0.70 0.26 0.59 0.41 0.83 ## PVD6 0.70 0.48 0.73 0.27 0.67 ## ## With Sums of squares of: ## g F1* F2* F3* ## 3.45 1.18 0.23 0.37 ## ## general/max 2.94 max/min = 5.09 ## mean percent general = 0.67 with sd = 0.13 and cv of 0.19 ## Explained Common Variance of the general factor = 0.66 ## ## The degrees of freedom are 3 and the fit is 0.01 ## The number of observations was 157 with Chi Square = 1.68 with prob &lt; 0.64 ## The root mean square of the residuals is 0.01 ## The df corrected root mean square of the residuals is 0.02 ## RMSEA index = 0 and the 10 % confidence intervals are 0 0.108 ## BIC = -13.49 ## ## Compare this with the adequacy of just a general factor and no group factors ## The degrees of freedom for just the general factor are 14 and the fit is 1.13 ## The number of observations was 157 with Chi Square = 171.53 with prob &lt; 3.4e-29 ## The root mean square of the residuals is 0.16 ## The df corrected root mean square of the residuals is 0.19 ## ## RMSEA index = 0.268 and the 10 % confidence intervals are 0.233 0.305 ## BIC = 100.74 ## ## Measures of factor score adequacy ## g F1* F2* F3* ## Correlation of scores with factors 0.89 0.81 0.52 0.74 ## Multiple R square of scores with factors 0.79 0.66 0.27 0.54 ## Minimum correlation of factor score estimates 0.57 0.33 -0.45 0.09 ## ## Total, General and Subset omega for each subset ## g F1* F2* F3* ## Omega total for total scores and subscales 0.94 0.92 0.74 0.91 ## Omega general for total scores and subscales 0.75 0.57 0.63 0.59 ## Omega group for total scores and subscales 0.16 0.35 0.11 0.32 #Scatterplot Matrix (ignoring stimuli variance) lqualtrics1 %&gt;% select(certainty, clarity, pvd_pinfect, pvd_germ, rchild_uncertain, rchild_ses, recentill) %&gt;% pairs.panels(scale = FALSE, pch = &quot;.&quot;) %&gt;% title(main = &quot;Scatterplot Matrix (ignoring stimuli variance) &quot;, line = 1.5) Figure 2.2: Scatterplot Matrix ignoring stimula variance (Michalak et al. 2020). I decided to also add a title to the scatter plot matrix (figure 2.2), since it was missing in the original code. Overall, it was not hard to generate these two figures using the authors’ script. I’d rate the effort it took to recreate the figures a 4.5/5, 5 being very easy. I immediately saw what had to be changed before even running the script and this only took a minute to do. The {GPArotation} package wasn’t in their list of libraries to load since it’s loaded automatically when running the omega() function. This doesn’t work when the package is not already installed, and running the code just produces an error. 2.3 Final rating I think this research article deserves a 9.5/10 for reproducibility. Chapters in the article describing ethics and data accessibility have their own headers so they’re easy to find in the table of contents, in the text itself every claim has a source, the data and scripts were one click away from the article itself and really the only thing making it not 10/10 reproducible was the absence of the raw data, which I assume is to protect participants’ privacy. References "],["datanalysis.html", "3 Analyzing and visualizing data 3.1 Preparing the data 3.2 Analyzing and visualizing the data", " 3 Analyzing and visualizing data The data that is to be visualized comes from an experiment involving C. elegans nematodes and was supplied by J. Louter of the INT/ILC. In this experiment, the number of offspring of adult C. elegans was counted after exposing them to different substances in multiple concentrations. 3.1 Preparing the data 3.1.1 Inspecting the data in R celegans_data &lt;- read_excel(here(&quot;data_raw/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) Before the data can be visualized, it has to be cleaned up. The dataframe is already in tidy format, but the data types of the columns need to be changed. Expected data types would be double/integer for RawData (number of offspring), character for compName (name of the compound) and double for compConcentration (concentration of the compound). The actual data types of the columns are double for RawData, character for compName and character for compConcentration. The data type for compConcentration has not been correctly assigned which will cause the following issue if it’s not changed: #Create a scatter plot plot_chr &lt;- ggplot(data = celegans_data, aes(x = compConcentration, y = RawData))+ geom_point(aes(color = compName, shape = expType), size = 1)+ labs(x = &quot;Concentration&quot;, y = &quot;Number of offspring&quot;, title = &quot;Number of C. elegans offspring under\\na number of circumstances, alphabetical&quot;, shape = &quot;Type&quot;, color = &quot;Compound&quot;)+ scale_colour_manual(values = c(&quot;red&quot;, &quot;darkgoldenrod1&quot;, &quot;green&quot;, &quot;royalblue3&quot;, &quot;violet&quot;))+ theme_minimal()+ theme(legend.key.size = unit(0.75,&quot;line&quot;), legend.text = element_text(size = 8), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) plot_chr Figure 3.1: Number of C. elegans offspring under a number of circumstances where the x-axis is ordered alphabetically The x-axis labels are ordered alphabetically because the data type of the compConcentration is character instead of double. R probably read it this way because Excel adds an E to showcase exponential values. 3.1.2 Fixing the character/double issue Since not all compounds share the same compUnit (the unit in which their concentration is measured), 2 separate plots need to be made and then combined: one for the compounds measured in nM and one for the compounds measured in percentage. After changing the data type from character to double and adding jitter to the plot to make it more readable it looks as follows: #Change data type from chr to dbl celegans_data$compConcentration &lt;- as.double(celegans_data$compConcentration) #Create a scatter plot for the concentration in nM celegans_data_nM &lt;- celegans_data %&gt;% filter(compUnit == &quot;nM&quot;) plot_nM &lt;- ggplot(data = celegans_data_nM, aes(x = log10(compConcentration), y = RawData))+ geom_jitter(aes(color = compName, #add jitter shape = expType), width = 0.5, height = 0.2)+ labs(x = &quot;log10 concentration (nM)&quot;, y = &quot;Number of offspring&quot;)+ coord_cartesian(ylim = c(0, 120))+ scale_shape_manual(values = 3)+ scale_colour_manual(values = c(&quot;red&quot;, &quot;darkgoldenrod1&quot;, &quot;royalblue3&quot;))+ theme_minimal()+ theme(legend.position = &quot;none&quot;) #Create a scatter plot for the concentration in pct celegans_data_pct &lt;- celegans_data %&gt;% filter(compUnit == &quot;pct&quot;) plot_pct &lt;- ggplot(data = celegans_data_pct, aes(x = expType, y = RawData))+ geom_jitter(aes(color = compName, #add jitter shape = expType), size = 1.3, width = 0.1, height = 0.075)+ xlab(&quot;Type&quot;)+ coord_cartesian(ylim = c(23.7, 121))+ scale_colour_manual(values = c(&quot;green&quot;, &quot;violet&quot;))+ theme_minimal()+ theme(legend.position = &quot;none&quot;, axis.text.x=element_text(vjust=0.5, hjust=0.5, size = 8.75), axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank())+ #remove any y-axis labeling rotate_axis_labels(&quot;x&quot;, 90) #obtain legend legend &lt;- get_legend(plot_chr) #combine figures and legend for readability ggdraw(plot_grid(plot_grid(plot_nM, plot_pct), plot_grid(NULL, legend, ncol = 3), rel_widths = c(1, 0.35)))+ plot_annotation(&quot;Number of C. elegans offspring under\\na number of circumstances&quot;) Figure 3.2: Total number of C. elegans offspring under a number of circumstances, with on the y-axis the number of offspring and on the x-axis the log10 concentration in nM. The positive control for this experiment is ethanol. The negative control for this experiment is S-medium. 3.2 Analyzing and visualizing the data 3.2.1 Plan for IC50/ED50 visualization Since this chapter is focused on visualization, there won’t be a statistical analysis of the dose-response interactions. If I were to perform a statistical analysis, it would be done using the {drc} package (Ritz et al. 2015), which has functions to calculate IC50s and ED50s, which then could be compared. Using {ggplot2} (Hadley Wickham 2016), a scatter plot can be made with on the y-axis the number of offspring and on the x-axis the compound concentration. The calculated IC50/ED50 values could be added as lines over the plot, giving every compound (line) a different color. An ANOVA can also be conducted to look for differences between concentrations, after checking if the data comes from a normal distribution using the Shapiro-Wilk test. 3.2.2 Normalizing the data To make is easier to read the results visualized in the plot, the number of offspring was normalized as a fraction of the negative control, meaning any value under 1 shows a reduction in the number of offspring. To normalize the data, the following code was used: #Obtain the mean of the RawData (negativeControl) controlNeg &lt;- celegans_data %&gt;% filter(compName == &quot;S-medium&quot;) %&gt;% summarize(mean = mean(RawData, na.rm = TRUE)) #Use the mean to calculate fractions mutated &lt;- celegans_data %&gt;% filter(RawData &gt; 0) %&gt;% select(RawData, compName, compConcentration, expType) %&gt;% na.omit() %&gt;% mutate(normalized = RawData/controlNeg$mean) #Obtain the means of the other 2 control groups controlPos &lt;- mutated %&gt;% filter(expType == &quot;controlPositive&quot;) %&gt;% summarize(mean = mean(normalized, na.rm = TRUE)) controlVeh &lt;- mutated %&gt;% filter(expType == &quot;controlVehicleA&quot;) %&gt;% summarize(mean = mean(normalized, na.rm = TRUE)) 3.2.3 Creating a legend To give the final figure a proper legend, the legend needs to be generated in a separate figure since we don’t want to include the test-compounds in it. The following code was used to generate a legend, making use of dummy data. Generating a legend using dummy data dummydf &lt;- data.frame(a = rep(c(1, 2, 3), each=2), #create dummy dataframe for the dummy plot b = c(1, 4, 6, 3, 4, 7), Control = rep(c(&quot;controlNegative&quot;, &quot;controlPositive&quot;, &quot;controlVehicleA&quot;), each=2)) dummyplot &lt;- ggplot(dummydf, aes(x=a, y=b, group=Control))+ #create dummy plot to obtain its legend geom_line(aes(linetype=Control, color=Control))+ scale_linetype_manual(values=c(&quot;solid&quot;, &quot;solid&quot;, &quot;longdash&quot;))+ scale_color_manual(values=c(&quot;violet&quot;, &quot;green&quot;, &quot;green&quot;)) dummylegend &lt;- get_legend(dummyplot) #obtain dummy legend 3.2.4 Generating the normalized plot With the data normalized and the legend obtained, the normalized plot can be generated. #create normalized plot plotfraction &lt;- mutated %&gt;% filter(compName == &quot;2,6-diisopropylnaphthalene&quot; | compName == &quot;decane&quot; | compName == &quot;naphthalene&quot;) %&gt;% ggplot(aes(x = log10(compConcentration), y = normalized))+ geom_jitter(aes(color = compName), width = 0.5, height = 0.1)+ scale_colour_manual(values = c(&quot;red&quot;, &quot;darkgoldenrod1&quot;, &quot;royalblue3&quot;))+ coord_cartesian(ylim = c(0, 1.5))+ labs(x = &quot;log10 concentration (nM)&quot;, y = &quot;Normalized number of offspring&quot;, title = &quot;Number of C. elegans offspring as a fraction\\nof the negative control group&quot;, color = &quot;Compound&quot;)+ geom_hline(yintercept = 1, color = &quot;violet&quot;)+ geom_hline(yintercept = controlPos$mean, color = &quot;green&quot;)+ geom_hline(yintercept = controlVeh$mean, color = &quot;green&quot;, linetype = &quot;longdash&quot;)+ theme_minimal()+ guides(color = &quot;none&quot;)+ facet_wrap(~ compName) #combine figure and legend ggdraw(plot_grid(plotfraction, dummylegend, rel_widths = c(5, 1))) Figure 3.3: Number of C. elegans offspring as a fraction of the negative control group, with on the y-axis the normalized number of offspring and on the x-axis the log10 concentration of the compounds in nM. This way it’s easier to read the graph. Everything below the pink line means less offspring than control C. elegans and everything above it means more. The figure (3.3) shows a decrease in the number of offspring for all 3 groups. References "],["drugscreening.html", "4 Drug Screening", " 4 Drug Screening Every year, 400,000 children and young people are diagnosed with cancer (Lewandowska et al. 2021). In developed countries, over 80% of children survive for at least 5 years after diagnosis (Schüz and Roman 2021), and surival rates keep going up (SteliarovaFoucher et al. 2004) as seen in figure 4.1. This does mean that around 20% of children don’t. Researchers all over the world are working on bringing this percentage down even more, including the doctors and researchers at the Princess Máxima Center in Utrecht. knitr::include_graphics(here(&quot;data/cancerstatistics.png&quot;)) Figure 4.1: Cancer survival rates for children in West and East Europe (SteliarovaFoucher et al. 2004). Princess Máxima Center is a children’s cancer research hospital, where all children diagnosed with cancer in the Netherlands are treated (Prinses Máxima Centrum n.d.). A core focus of the Princess Máxima Center is combining treatment with research and vice versa, which how new methods for treatment are found. When treating cancer, it’s important to use a drug that both kills the tumor and has the lowest number of side effects. Specialized cancer hospitals (like the Princess Máxima Center) run tests to determine which drug this would be, using a high throughput facility that can screen hundreds of different drugs used on patient material at once (“High-Throughput Screening (HTS)” n.d.). In theory, this could be done for every patient, meaning they could be treated with the most suitable drug for their type of cancer. Where the pipeline for single-drug testing is established, there is no set way to analyse the data obtained from a combination-drug screening. When doing a combination-drug screening, multiple drugs are combined. Researchers hope to find combinations of drugs that have a greater effect in combination with each other than when their separate effects are added together, see figure 4.2. Synergy, as this is called, is desired, so patients can experience the same positive effects but less side effects from the drugs (Cokol 2012). knitr::include_graphics(here(&quot;data/synergy_antagonism.png&quot;)) Figure 4.2: Different types of interactions between drugs (Cokol 2012). However, analyzing combination-screen data is more complex than analyzing single-screen data (Rounsaville, Petry, and Carroll 2003). It is up to me and 2 of my fellow students to extend the existing pipeline to make is simpler for researchers to visualize and interpret their data from combination-screens, focusing on the different types of synergy (Jaaks et al. 2022). The R package {SynergyFinder} (Zheng et al. 2022) contains a multitude of functions to visualize dose-response and synergy data and will be used in the package that we are going to create ourselves. The main focus will be on creating a function (or multiple functions that go together) that can filter the data from the high throughput facility for the “best” drug combinations, after the data has been molded to fit into the {SynergyFinder} functions. It is important that every parameter can be set manually instead of being hard-coded into the function, so that the function can be used to help answer any research question, instead of being specific for one single situation. With this package will come a DESCRIPTION file containing metadata (like the package title and version for example), a NAMESPACE file that ensures R will be able to find every function written, a vignette (guide) and of course all the functions we think are necessary for the data analysis of combination-drug screening data. References "],["working-with-relational-databases.html", "5 Working with relational databases 5.1 Preparing the data 5.2 Inspecting the data 5.3 Combining the dataframes 5.4 Visualizing the data", " 5 Working with relational databases Having spent so much time analyzing data, it would be useful to learn more about obtaining data from databases, and how to manipulate and analyze relational data using SQL. 5.1 Preparing the data 5.1.1 Introduction In this report I will be analyzing 2 datasets obtained from Google Dengue Trends and Google Flu Trends. Sadly, the metadata is incomplete and I am not sure what the numbers mean, it’s not documented. I will be assuming that the datasets contain the number of dengue and flu cases per week of the year, from 2002 through 2015. This shows just how important complete metadata is. 5.1.2 Importing the data and making it tidy The first step in the data analysis is to import the data into Rstudio and make it tidy, meaning each variable gets its own column, each observation gets its own row and each value its own cell. In this case neither of the datasets were in tidy format, as there were multiple observations in each row. After making the data tidy, the class of every column was checked and changed to match the class of that column in the gapminder ({dslabs}, (Irizarry and Gill 2021)) dataframe, so they can later be joined together. A “year” column was added to both the dengue and the flu data as well, to match the “year” column of gapminder. dengue_data &lt;- read_csv(here(&quot;data_raw/dengue_data.csv&quot;), skip = 11) #first 11 rows contain metadata flu_data &lt;- read_csv(here(&quot;data_raw/flu_data.csv&quot;), skip = 11) #first 11 rows contain metadata dengue_tidy &lt;- pivot_longer(data = dengue_data, cols = c(2:ncol(dengue_data)), names_to = &quot;country&quot;, values_to = &quot;cases&quot;) #each observation must have its own row flu_tidy &lt;- pivot_longer(data = flu_data, cols = c(2:ncol(flu_data)), names_to = &quot;country&quot;, values_to = &quot;cases&quot;) #each observation must have its own row dengue_tidy &lt;- dengue_tidy %&gt;% mutate(&quot;year&quot; = substr(Date, 1, 4)) #add &quot;year&quot; column dengue_tidy$year &lt;- as.integer(dengue_tidy$year) #change class of column year from chr to int flu_tidy &lt;- flu_tidy %&gt;% mutate(&quot;year&quot; = substr(Date, 1, 4)) #add &quot;year&quot; column flu_tidy$year &lt;- as.integer(flu_tidy$year) #change class of column year from chr to int dengue_tidy$country &lt;- as.factor(dengue_tidy$country) #the column country in gapminder is of the class factor flu_tidy$country &lt;- as.factor(flu_tidy$country) #the column country in gapminder is of the class factor The tidy dataframes were then saved as separate rds and csv files: saveRDS(gapminder, here(&quot;data/gapminder.rds&quot;)) #store the three tables as .rds and .csv files saveRDS(dengue_tidy, here(&quot;data/dengue.rds&quot;)) saveRDS(flu_tidy, here(&quot;data/flu.rds&quot;)) write_csv(gapminder, here(&quot;data/gapminder.csv&quot;)) write_csv(dengue_tidy, here(&quot;data/dengue.csv&quot;)) write_csv(flu_tidy, here(&quot;data/flu.csv&quot;)) 5.1.3 Connecting to the database For this analysis, a PostgreSQL database was created in Dbeaver. To connect to the database in Rstudio, the {RPostgres} (H. Wickham, Ooms, and Müller 2022) package was used. con &lt;- dbConnect(Postgres(), #connect to the database dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;password&quot;) 5.1.4 Exporting the tidy dataframes to the database dbWriteTable(con, &quot;gapminder&quot;, gapminder) #insert the tables into the database dbWriteTable(con, &quot;dengue&quot;, dengue_tidy) dbWriteTable(con, &quot;flu&quot;, flu_tidy) 5.2 Inspecting the data 5.2.1 Inspecting the data using SQL To make sure no important data is missing, the tables were checked for NULL values. Data I consider important is the year and the country data, since if there are any missing values here errors may occur. Checking for NULL values SELECT * FROM gapminder WHERE year IS NULL; --make sure no important values are missing Table 5.1: 0 records country year infant_mortality life_expectancy fertility population gdp continent region SELECT * FROM gapminder WHERE country IS NULL; Table 5.2: 0 records country year infant_mortality life_expectancy fertility population gdp continent region SELECT * FROM dengue WHERE year IS NULL; Table 5.3: 0 records Date country cases year SELECT * FROM dengue WHERE country IS NULL; Table 5.4: 0 records Date country cases year SELECT * FROM flu WHERE year IS NULL; Table 5.5: 0 records Date country cases year SELECT * FROM flu WHERE country IS NULL; Table 5.6: 0 records Date country cases year There were no records for any of these selects, meaning there are no NULL values anywhere in “year” or “country”. Now to actually look at the data, there were a couple of statistics I was interested in, like: How many observations were done SELECT COUNT(cases) --count the number of observations (ignoring NULL) FROM dengue; Table 5.7: 1 records count 6263 SELECT COUNT(cases) FROM flu; Table 5.8: 1 records count 17266 The total number of cases SELECT SUM(cases) --calculate the total number of cases FROM dengue; Table 5.9: 1 records sum 870.376 SELECT SUM(cases) FROM flu; Table 5.10: 1 records sum 8179518 The countries with the highest number of cases SELECT year, country, cases*1000 AS cases1000 --look at the highest observed numbers of cases FROM dengue WHERE cases IS NOT NULL ORDER BY cases1000 DESC; Table 5.11: Displaying records 1 - 10 year country cases1000 2008 Brazil 1000 2004 Indonesia 1000 2012 India 1000 2009 Mexico 1000 2010 Philippines 1000 2009 Bolivia 1000 2013 Thailand 1000 2009 Argentina 1000 2010 Venezuela 1000 2005 Singapore 1000 SELECT year, country, cases FROM flu WHERE cases IS NOT NULL ORDER BY cases DESC; Table 5.12: Displaying records 1 - 10 year country cases 2013 United States 10555 2013 United States 10112 2009 Canada 9688 2009 Canada 9630 2013 United States 9408 2012 United States 8618 2003 United States 8196 2012 United States 7904 2013 Canada 7698 2003 Canada 7531 5.2.2 Inspecting the data using R Now it’s time to inspect the data in R. When importing the dengue data from its csv, I noticed every value in the dataframe was a fraction instead of a whole number. Since no Readme was provided with the data, I am not sure why this is. To make the values whole, I multiplied the number of cases by 1000. To find out which countries had the highest and lowest observed number of cases, the following code was used: dengue_mutated &lt;- dengue_tidy %&gt;% mutate(&quot;cases1000&quot; = cases * 1000) #add a column with the multiplied number of cases maxcasesdengue &lt;- subset(dengue_mutated, cases1000 == max(na.omit(dengue_mutated$cases1000))) mincasesdengue &lt;- subset(dengue_mutated, cases1000 == min(na.omit(dengue_mutated$cases1000))) unique(maxcasesdengue$country) #look at which countries have the highest observed number of dengue cases ## [1] Indonesia Singapore Brazil Bolivia Argentina Mexico ## [7] Venezuela Philippines India Thailand ## 10 Levels: Argentina Bolivia Brazil India Indonesia Mexico ... Venezuela unique(mincasesdengue$country) #look at which countries have the lowest observed number of dengue cases ## [1] India Bolivia ## 10 Levels: Argentina Bolivia Brazil India Indonesia Mexico ... Venezuela maxcasesflu &lt;- subset(flu_tidy, cases == max(na.omit(flu_tidy$cases))) mincasesflu &lt;- subset(flu_tidy, cases == min(na.omit(flu_tidy$cases))) unique(maxcasesflu$country) #look at which countries have the highest observed number of flu cases ## [1] United States ## 29 Levels: Argentina Australia Austria Belgium Bolivia Brazil ... Uruguay unique(mincasesflu$country) #look at which countries have the lowest observed number of flu cases ## [1] Chile Sweden ## 29 Levels: Argentina Australia Austria Belgium Bolivia Brazil ... Uruguay Another statistic is the total number and the average number of cases per country of all years combined. dengue_country_means &lt;- dengue_mutated %&gt;% group_by(country) %&gt;% summarize(total = sum(cases1000, na.rm = TRUE), mean = mean(cases1000, na.rm = TRUE)) %&gt;% arrange(desc(mean)) #calculate the total number of cases and mean number of cases per country flu_country_means &lt;- flu_tidy %&gt;% group_by(country) %&gt;% summarize(total = sum(cases, na.rm = TRUE), mean = mean(cases, na.rm = TRUE)) %&gt;% arrange(desc(mean)) Dengue numbers (#tab:dengue dataframemeans)Table containing dengue numbers per country country total mean Venezuela 180893 277.44325 Thailand 66761 184.93352 Indonesia 97124 147.38088 Singapore 94747 143.77390 Brazil 93638 142.09105 Mexico 92989 141.53577 Philippines 88106 136.38700 India 76889 116.67527 Argentina 49779 76.34816 Bolivia 29450 44.68892 Flu numbers Table 5.13: Table containing flu numbers per country country total mean South Africa 1349388 2698.776000 Canada 1169673 1886.569355 United States 1142909 1843.401613 Mexico 703432 1134.567742 Austria 500332 806.987097 Germany 496073 800.117742 Romania 463130 709.234303 Bulgaria 331824 595.734291 Russia 262781 463.458554 Australia 219016 436.286853 Ukraine 204308 397.486381 Bolivia 192202 295.241167 Japan 160897 261.196429 Paraguay 135348 253.460674 Peru 144810 219.742033 Brazil 138825 210.660091 Argentina 100391 153.503058 Belgium 76869 123.982258 Uruguay 76395 117.893518 France 61625 99.395161 Hungary 50572 90.146168 Norway 40462 72.124777 Switzerland 43573 70.850407 Poland 30456 53.619718 Spain 31856 51.380645 New Zealand 19595 39.033864 Netherlands 22408 36.200323 Chile 7544 11.482496 Sweden 2824 5.548134 Though interesting, these numbers only represent the total number of cases, not taking into account the size of the difference in population between countries. That is where the gapminder dataframe comes into play, containing exactly the data needed to include the population in the calculations. 5.3 Combining the dataframes 5.3.1 Preparing the tidy dataframes To prepare the dataframes for the joining NA values were removed, and both the sum and the weekly mean of the number of cases were added. #calculate the number of cases per year and the average number of cases per week per country dengue_combine &lt;- na.omit(dengue_mutated) %&gt;% group_by(country, year) %&gt;% summarize(total_cases_dengue = sum(cases1000), mean_weekly_dengue = mean(cases1000)) flu_combine &lt;- na.omit(flu_tidy) %&gt;% group_by(country, year) %&gt;% summarize(total_cases_flu = sum(cases), mean_weekly_flu = mean(cases)) 5.3.2 Joining the dataframes together The new dataframes were then exported to the database, where they were joined using SQL: dbWriteTable(con, &quot;dengue_combine&quot;, dengue_combine) #insert the new tables into the database dbWriteTable(con, &quot;flu_combine&quot;, flu_combine) CREATE TABLE dengue_gapminder AS --combine dengue_data and gapminder SELECT gapminder.country, gapminder.year, gapminder.population, gapminder.continent, gapminder.region, dengue_combine.total_cases_dengue, dengue_combine.mean_weekly_dengue FROM gapminder LEFT JOIN dengue_combine ON gapminder.country = dengue_combine.country AND gapminder.year = dengue_combine.year; CREATE TABLE flu_gapminder AS --combine flu_data and gapminder SELECT gapminder.country, gapminder.year, gapminder.population, gapminder.continent, gapminder.region, flu_combine.total_cases_flu, flu_combine.mean_weekly_flu FROM gapminder LEFT JOIN flu_combine ON gapminder.country = flu_combine.country AND gapminder.year = flu_combine.year; CREATE TABLE flu_gapminder_dengue AS --combine all 3 tables SELECT flu_gapminder.country, flu_gapminder.year, flu_gapminder.population, flu_gapminder.continent, flu_gapminder.region, flu_gapminder.total_cases_flu, flu_gapminder.mean_weekly_flu, dengue_combine.total_cases_dengue, dengue_combine.mean_weekly_dengue FROM flu_gapminder INNER JOIN dengue_combine ON flu_gapminder.country = dengue_combine.country AND flu_gapminder.year = dengue_combine.year; 5.3.3 Loading the combined tables into R dengue_gapminder &lt;- dbReadTable(con, &quot;dengue_gapminder&quot;) #load the combined tables into R flu_gapminder &lt;- dbReadTable(con, &quot;flu_gapminder&quot;) flu_gapminder_dengue &lt;- dbReadTable(con, &quot;flu_gapminder_dengue&quot;) dengue_gapminder_nona &lt;- na.omit(dengue_gapminder) #remove NA values flu_gapminder_nona &lt;- na.omit(flu_gapminder) flu_gapminder_dengue_nona &lt;- na.omit(flu_gapminder_dengue) 5.4 Visualizing the data 5.4.1 Normalizing the data Now that the dataframes are combined, the gapminder column “population” can be used to normalize the data. We now know the average number of weekly cases per 100.000 population, of every country per year. #normalize the data by calculating the average number of cases per 100,000 population dengue_gapminder_normalized &lt;- dengue_gapminder_nona %&gt;% mutate(&quot;normalized_dengue&quot; = (total_cases_dengue/population)*100000, &quot;normalized_dengue_mean&quot; = (mean_weekly_dengue/population)*100000) flu_gapminder_normalized &lt;- flu_gapminder_nona %&gt;% mutate(&quot;normalized_flu&quot; = (total_cases_flu/population)*100000, &quot;normalized_flu_mean&quot; = (mean_weekly_flu/population)*100000) flu_gapminder_dengue_normalized &lt;- flu_gapminder_dengue_nona %&gt;% mutate(&quot;normalized_dengue&quot; = (total_cases_dengue/population)*100000, &quot;normalized_flu&quot; = (total_cases_flu/population)*100000, &quot;normalized_dengue_mean&quot; = (mean_weekly_dengue/population)*100000, &quot;normalized_flu_mean&quot; = (mean_weekly_flu/population)*100000) 5.4.2 Calculating descriptive statistics In an attempt to find some descriptive statistics, I tried calculating the means, standard deviations and Shapiro-Wilk p-values (to see if the data is from a normal distribution). #calculate means, standard deviations and Shapiro-Wilk p-values per region dengue_gap_summary &lt;- dengue_gapminder_normalized %&gt;% group_by(region) %&gt;% summarize(mean = mean(total_cases_dengue), sd = sd(total_cases_dengue), pvalue_sw = shapiro.test(total_cases_dengue)$p.value) flu_gap_summary &lt;-flu_gapminder_normalized %&gt;% group_by(region) %&gt;% summarize(mean = mean(total_cases_flu), sd = sd(total_cases_flu), pvalue_sw = shapiro.test(total_cases_flu)$p.value) #calculate means, standard deviations and Shapiro-Wilk p-values per country dengue_gap_summary_country &lt;- dengue_gapminder_normalized %&gt;% group_by(country) %&gt;% summarize(mean = mean(total_cases_dengue), sd = sd(total_cases_dengue), pvalue_sw = shapiro.test(total_cases_dengue)$p.value) flu_gap_summary_country &lt;-flu_gapminder_normalized %&gt;% group_by(country) %&gt;% summarize(mean = mean(total_cases_flu), sd = sd(total_cases_flu), pvalue_sw = shapiro.test(total_cases_flu)$p.value) Not every country has a normal distribution (normal distribution is where p &gt; 0.05). After finding the countries that do, levene’s test was performed to check for equal variances: #find the countries where the data is a normal distribution countries_normal_dengue &lt;- subset(dengue_gap_summary_country, pvalue_sw &gt; 0.05)[,1] %&gt;% pull() countries_normal_flu &lt;- subset(flu_gap_summary_country, pvalue_sw &gt; 0.05)[,1] %&gt;% pull() #filter for the rows with observations from those countries dengue_normal &lt;- dengue_gapminder_normalized[dengue_gapminder_normalized$country %in% countries_normal_dengue,] flu_normal &lt;- flu_gapminder_normalized[flu_gapminder_normalized$country %in% countries_normal_flu,] #perform levene&#39;s test leveneTest(total_cases_dengue ~ country, data = dengue_normal, center = mean)[1,3] ## [1] 0.1828471 leveneTest(total_cases_flu ~ country, data = flu_normal, center = mean)[1,3] ## [1] 5.411843e-33 5.4.3 Visualizing correlation After calculating these p-values, I realized I’m more interested in correlation than difference between the groups, since it’s already known the groups are differt from each other (different countries). Instead of performing an ANOVA or Kruskal-Wallis test, it was decided that the pearson’s correlation coefficient would be calculated. A scatter plot was generated to visualize correlation: #perform a pearson correlation analysis dengue_flu_cor &lt;- round(cor.test(flu_gapminder_dengue_normalized$total_cases_dengue, flu_gapminder_dengue_normalized$total_cases_flu, method = &quot;pearson&quot;)$estimate, 3) cor_pvalue &lt;- cor.test(flu_gapminder_dengue_normalized$total_cases_dengue, flu_gapminder_dengue_normalized$total_cases_flu, method = &quot;pearson&quot;)$p.value #visualize the correlation in a scatter plot ggplot(data = flu_gapminder_dengue_normalized, aes(y = total_cases_dengue, x = total_cases_flu)) + geom_point()+ labs(title = &quot;Relation between dengue and flu cases&quot;, subtitle = paste(&quot;pearson&#39;s r = &quot;, dengue_flu_cor), y = &quot;Number of dengue cases&quot;, x = &quot;Number of flu cases&quot;)+ theme_minimal() Figure 5.1: Scatter plot showing the correlation between the number of dengue and flu cases, with on the y-axis the number of dengue cases and on the x-axis the number of flu cases. Each dot represents a year of observations from one country. There is a significant correlation between the number of dengue and flu cases in a country, as p is smaller than 0.05 at cor_pvalue. The correlation coefficient of dengue_flu_cor indicates a weak correlation (Taylor 1990). 5.4.4 Visualizing the dengue data Dengue is endemic in most tropical countries / regions (Gubler 2002), as shown below: #visualize the number of dengue cases per region in a bar plot ggplot(dengue_gap_summary, aes(x = region, y = mean, fill = region))+ geom_col()+ geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd, width = 0.2))+ labs(title = &quot;Number of dengue cases per region&quot;, subtitle = &quot;Error bars depict 1 standard deviation&quot;, x=&quot;Region&quot;, y=&quot;Average number of dengue cases&quot;)+ theme_minimal()+ theme(legend.position = &quot;none&quot;) Figure 5.2: Bar plot showing the number of dengue cases per region, with on the y-axis the yearly average number of dengue cases. This graph was made by looking at the average number of cases per region per year, meaning it uses the total number of cases rather that the normalized ones that were calculated earlier. A more representative plot would use these numbers: #create a scatter plot of the dengue data dengue_plot &lt;- ggplot(dengue_gapminder_normalized, aes(x = year, y = mean_weekly_dengue, group = country, color = country))+ geom_line()+ geom_point()+ labs(title = &quot;Weekly dengue cases per country&quot;, y = &quot;Average number of dengue cases per week\\nper 100,000 population&quot;, x = &quot;Year&quot;, color = &quot;Country&quot;)+ scale_x_continuous(breaks = seq(from = min(dengue_gapminder_normalized$year), to = max(dengue_gapminder_normalized$year), by = 1))+ theme_minimal() ggplotly(dengue_plot) #visualize the number of dengue cases per country in an interactive scatter plot Figure 5.3: Interactive scatter plot showing the normalized number of dengue cases per country, with on the y-axis the average number of dengue cases per week per 100,000 population and on the x-axis the year. 5.4.5 Visualizing the flu data The same graphs can be made using the flu data: #visualize the number of flu cases per region in a bar plot ggplot(flu_gap_summary, aes(x = region, y = mean, fill = region)) + geom_col()+ geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd, width = 0.2))+ labs(title = &quot;Number of flu cases per region&quot;, subtitle = &quot;Error bars depict 1 standard deviation&quot;, x=&quot;Region&quot;, y=&quot;Average number of flu cases&quot;)+ theme_minimal()+ theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) Figure 5.4: Bar plot showing the number of flu cases per region, with on the y-axis the yearly average number of flu cases. This graph has the same issue as before, using the total number of cases rather than the normalized data. Northern America and Southern Africa stand out with their high number of cases. It is true that Northern America has a high number of weekly flu cases, but seeing their large population this was to be expected. More interesting is Southern Africa, where the population is smaller but the number of cases is higher. Looking at some random observations: SELECT DISTINCT country, region FROM flu_gapminder WHERE region = &#39;Southern Africa&#39; AND total_cases_flu IS NOT NULL Table 5.14: 1 records country region South Africa Southern Africa South Africa is the only country in region Southern Africa with flu data. SELECT year, country, cases FROM flu WHERE country = &#39;South Africa&#39; AND cases IS NOT NULL ORDER BY RANDOM(); Table 5.15: Displaying records 1 - 10 year country cases 2010 South Africa 2350 2010 South Africa 3969 2009 South Africa 2515 2014 South Africa 4253 2014 South Africa 2792 2007 South Africa 2896 2006 South Africa 3002 2011 South Africa 3143 2010 South Africa 1628 2015 South Africa 2677 SELECT year, country, cases FROM flu WHERE cases IS NOT NULL ORDER BY RANDOM(); Table 5.16: Displaying records 1 - 10 year country cases 2006 South Africa 2312 2014 Spain 12 2013 Argentina 238 2013 Ukraine 555 2010 Canada 570 2013 Austria 1060 2012 Russia 452 2008 Switzerland 91 2013 Ukraine 543 2009 Brazil 250 We can see that the number of weekly cases in South Africa is very high compared to other countries, causing it to stand out when looking at the bar graph. This will also be reflected in the average number of weekly cases: #create a scatter plot of the flu data flu_plot &lt;- ggplot(flu_gapminder_normalized, aes(x = year, y = mean_weekly_flu, group = country, color = country))+ geom_line()+ geom_point()+ labs(title = &quot;Weekly flu cases per country&quot;, y = &quot;Average number of flu cases per week\\nper 100,000 population&quot;, x = &quot;Year&quot;, color = &quot;Country&quot;)+ scale_x_continuous(breaks = seq(from = min(flu_gapminder_normalized$year), to = max(flu_gapminder_normalized$year), by = 1))+ theme_minimal() ggplotly(flu_plot) #visualize the number of flu cases per country in an interactive scatter plot Figure 5.5: Interactive scatter plot showing the normalized number of flu cases per country, with on the y-axis the average number of flu cases per week per 100,000 population and on the x-axis the year. Northern American countries United States and Canada still stand out, having higher weekly averages than most countries, despite the numbers being normalized for this graph. South Africa does have the highest average number of flu cases per week per 100,000 population since data started being collected from there. Now that the analysis and visualization is done, Rstudio can be disconnected from the database. dbDisconnect(con) References "],["parameterized.html", "6 Parameterized report on COVID-19 6.1 Parameterized reports 6.2 The parameterized COVID-19 report", " 6 Parameterized report on COVID-19 6.1 Parameterized reports Up until now, every chapter has been a static report. When using a dataset with, for example, a large number of groups, it would be nice to able to choose what groups exactly you want to visualize with just the click of a button. This is what parameterized reports are for. In this report I will be visualizing COVID-19 data (obtained from ECDC), more specifically the daily numbers of newly reported COVID-19 cases and deaths in EU countries. 6.2 The parameterized COVID-19 report 6.2.1 The parameters The parameters I will be using are country, year and month. Using these parameters, I will generate two interactive plots: one for the newly reported number of cases and one for the newly reported number of deaths. 6.2.2 Rendering the report The parameters are chosen when rendering the report, and are set as follows: #generate the report with chosen parameters rmarkdown::render(&quot;06_parameterizedcovid.Rmd&quot;, params = list(country = c(&quot;Germany&quot;, &quot;France&quot;, &quot;Netherlands&quot;), year = 2022, month = 1:3)) For the parameterization to work, the YAML header of the markdown file needs to contain some defaults for the parameters, that are used when the parameter is not specified when rendering. --- params: country: &quot;Austria&quot; year: 2022 month: 10 --- library(tidyverse) library(plotly) library(scales) library(here) data &lt;- read.csv(here(&quot;data_raw/data.csv&quot;)) #load the data After the data is loaded, it can be filtered to reflect the given parameters. I will also be determining the axis breaks depending on how many years are plotted, to keep the x-axis readable. data_filtered &lt;- data %&gt;% filter(countriesAndTerritories %in% params$country, year %in% params$year, month %in% params$month) #filter for the given parameters data_filtered &lt;- mutate(data_filtered, &quot;date&quot; = paste(day, month, year, sep=&quot;/&quot;)) #add date column data_filtered$date &lt;- as.Date(data_filtered$date, format=&quot;%d/%m/%Y&quot;) #change the data type to date if(length(params$year) &gt; 1){ #determine the x-axis breaks datebreaks &lt;- &quot;1 month&quot; }else{ datebreaks &lt;- &quot;2 weeks&quot; } Finally, the data can be plotted: plot_cases &lt;- ggplot(data_filtered, aes(x = date, y = cases, group = countriesAndTerritories, color = countriesAndTerritories))+ geom_line()+ geom_point(size = 1)+ labs(title = &quot;Number of newly reported COVID-19 cases over time by country&quot;, y = &quot;Number of COVID-19 cases&quot;, x = &quot;Date&quot;, color = &quot;Country&quot;)+ scale_x_date(date_breaks = datebreaks, date_labels = &quot;%d-%m-%y&quot;)+ scale_y_continuous(labels = label_comma())+ theme_minimal()+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) ggplotly(plot_cases) #generate an interactive plot showing the number of cases Figure 6.1: Number of newly reported COVID-19 cases over time by country, with on the y-axis the number of COVID-19 cases and on the x-axis the date. plot_deaths &lt;- ggplot(data_filtered, aes(x = date, y = deaths, group = countriesAndTerritories, color = countriesAndTerritories))+geom_line()+ geom_point(size = 1)+ labs(title = &quot;Number of newly reported COVID-19 deaths over time by country&quot;, y = &quot;Number of COVID-19 deaths&quot;, x = &quot;Date&quot;, color = &quot;Country&quot;)+ scale_x_date(date_breaks = datebreaks, date_labels = &quot;%d-%m-%y&quot;)+ scale_y_continuous(labels = label_comma())+ theme_minimal()+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) ggplotly(plot_deaths) #generate an interactive plot showing the number of deaths Figure 6.2: Number of newly reported COVID-19 deaths over time by country, with on the y-axis the number of COVID-19 deaths and on the x-axis the date. 6.2.3 Using different parameters Because the report is parameterized, it’s simple to recreate the graph for different countries and time periods: #generate the plots with different parameters rmarkdown::render(&quot;06_parameterizedcovid.Rmd&quot;, params = list(country = c(&quot;Estonia&quot;, &quot;Latvia&quot;, &quot;Lithuania&quot;), year = 2020:2021, month = 1:12)) Figure 6.3: Number of newly reported COVID-19 cases over time by country, with on the y-axis the number of COVID-19 cases and on the x-axis the date, this time using different parameters. Figure 6.4: Number of newly reported COVID-19 deaths over time by country, with on the y-axis the number of COVID-19 deaths and on the x-axis the date, this time using different parameters. "],["vignette.html", "7 R package vignette 7.1 Creating the package 7.2 Installing and loading the package 7.3 Package contents 7.4 Additional files", " 7 R package vignette 7.1 Creating the package I have created a package to help me with data analysis. My package {dsfbnine} was created with the help of the {devtools} (Hadley Wickham et al. 2022) and {usethis} (Hadley Wickham, Bryan, and Barrett 2022) packages. The general frame was made using function from these packages, like usethis::create_package and devtools::document. The functions included in this package are specific to my own workflow, most were written by looking at my own duplicate code. The main use of the functions are to make my code tidier and help me work faster, since I often do things like plotting illness cases for example. The rest of this chapter will be the vignette that comes with the package. 7.2 Installing and loading the package devtools::install_github(&quot;nineluijendijk/dsfbnine&quot;) library(dsfbnine) 7.3 Package contents 7.3.1 Dataset measlesdata The {dsfbnine} package contains some functions to help me with my data analysis and one cleaned dataset measlesdata. It contains data on measles cases and population numbers in EU countries from 2018 and 2019. The dataset loads as a dataframe with 744 rows and 8 variables. It is available via lazy-loading, meaning columns can be accessed as follows: measlesdata$cases %&gt;% head(n = 20) ## [1] 7 5 15 17 12 6 1 3 6 0 4 1 1 6 9 11 24 22 3 11 7.3.2 Function dataSummarizer() The function dataSummarizer() calculates the sum, mean, median and standard deviation of data, grouped by a given argument: dataSummarizer(measlesdata, groupBy = c(&quot;CountryName&quot;, &quot;year&quot;)) %&gt;% head(n = 10) ## # A tibble: 10 × 6 ## # Groups: CountryName [7] ## CountryName year total mean median standarddeviation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EU/EEA 2019 13207 1101. 1012. 783. ## 2 EU/EEA 2018 12352 1029. 959 687. ## 3 France 2018 2913 243. 136 252. ## 4 France 2019 2636 220. 166. 176. ## 5 Italy 2018 2517 210. 170 155. ## 6 Greece 2018 2293 191. 96.5 211. ## 7 Romania 2019 1706 142. 111 72.8 ## 8 Italy 2019 1626 136. 162 106. ## 9 Poland 2019 1423 119. 82.5 121. ## 10 Bulgaria 2019 1235 103. 46.5 110. 7.3.3 Function illnessPlot() The function illnessPlot() generates a graph showing illness numbers, specifically COVID-19 numbers from an ECDC dataset, used in Chapter 6 of my portfolio. The same code was used to generate 4 different plots, to fix this issue illnessPlot() was created. coviddata &lt;- read.csv(here::here(&quot;data/datacovid.csv&quot;)) #load the data illnessPlot(coviddata, countries = c(&quot;Netherlands&quot;, &quot;Belgium&quot;, &quot;France&quot;), years = 2020:2022, parameter = cases) Figure 7.1: Example illness plot. 7.3.4 Function fastaImporter.R The function fastaImporter() can be used to import FASTA sequences by entering their GenBank Identifiers. The imported sequences will be stored in an object of class “DNAbin”. dna &lt;- fastaImporter(here::here(&quot;data/species.txt&quot;)) dna ## 16 DNA sequences in binary format stored in a list. ## ## Mean sequence length: 16523.81 ## Shortest sequence: 16264 ## Longest sequence: 17062 ## ## Labels: ## KF914213.1 Gorilla beringei graueri mitochondrion, complete ... ## NC_011120.1 Gorilla gorilla gorilla mitochondrion, complete ... ## GU170821.1 Homo sapiens isolate NS07 mitochondrion, complete... ## HM015213.1 Pan paniscus isolate PP25 mitochondrion, complete... ## NC_001643.1 Pan troglodytes mitochondrion, complete genome ## NC_002083.1 Pongo abelii mitochondrion, complete genome ## ... ## ## Base composition: ## a c g t ## 0.314 0.299 0.131 0.256 ## (Total: 264.38 kb) 7.3.5 Function gotermAnalysis() The function gotermAnalysis() was written specifically to help with my analysis of RNA-sequencing data, which is why there are many different parameters to be set: changing just 1 can completely change the results (seen in the code chunk below). The function performs a GO-term enrichment analysis. The output of this function can be used in the next function gotermPlot() as well. DESeqresults &lt;- readRDS(here::here(&quot;data/dge_results&quot;)) #load the data object of class &quot;DESeqResults&quot; gotermAnalysis_results &lt;- gotermAnalysis(DESeqresults) 7.3.6 Function gotermPlot() The function gotermPlot() is an extension to the previous gotermAnalysis() function, generating a figure containing the most abundant GO-terms. To save computing time, the gotermAnalysis() function will not be ran. This means its output has to be imported for the gotermPlot() function: gotermAnalysis_results &lt;- readRDS(here::here(&quot;data/gotermAnalysis_results&quot;)) gotermPlot(gotermAnalysis_results, topamount = 10, plot_title = &quot;Top 10 upregulated GO-terms&quot;) Figure 7.2: Example GO-term plot. 7.4 Additional files Additional files can be found in the /inst/extdata/ directory. It consist of files used to build the vignette, (raw) data for analysis/plotting, example files and an R script that shows how the dataset measlesdata was cleaned. References "],["primates.html", "8 Generating a phylogenetic tree in R 8.1 The plan 8.2 Preparatory research 8.3 Getting started 8.4 Generating the first tree 8.5 Different types of phylogenetic models", " 8 Generating a phylogenetic tree in R 8.1 The plan Starting February 2025 I want to be doing my master’s in Biology at the University of Western Australia. I would like to specialize in Evolutionary Biology and zoology. During my minor I attended classes at Radboud University together with other (pre-master) students in Biology - Adaptive Organisms to get a head start on learning about evolutionary biology, both because it’s something I enjoy and to make the transition from Life Sciences to a master’s program easier. I would like to learn more about the differences between multiple primates and generate a phylogenetic tree. Prior to me starting on the actual code for this phylogenetic tree, I had to look into the different types of trees and how to generate them. 8.2 Preparatory research I started by looking for R packages having to do with phylogenetic research. There were ample packages to help generate phylogenetic trees, but none of them seemed to have all the tools needed. Wanting to keep my research as reproducible as possible, my main priority was to keep every single part of the research in R, which is free and open source software (R Core Team 2014). To get an idea of how to get started, I read a couple of papers by researchers who’ve created phylogenetic trees. In these papers, most of the software used to generate the trees (and all the steps that come before) was not R (Pozzi et al. 2014); (Perelman et al. 2011). Some of the programs were point and click as well (Suchard et al. 2018); (Vaidya, Lohman, and Meier 2011), making the research less reproducible. After looking at some different R packages I decided {ape} (Paradis and Schliep 2019) was the best option, seeing as most packages for phylogenetic research are dependent on it (Jombart and Ahmed 2011). The first step in the analysis was to obtain the data from an online database. NCBI has their own command-line tool called NCBI Datasets (NCBI Staff 2022). The command-line tool was installed using Miniconda3 (Anaconda n.d.). The tool did work fine, but it wasn’t exactly what I was looking for. This was also when it was decided to perform the analysis on the mitochondrial genomes of the primates, instead of the nuclear genomes, due to computational limitations. 8.3 Getting started I ended up making a list of the species of interest, containing their GenBank (Benson et al. 2013) identifier, scientific name and their common name, and saving it in a text file called species.txt. This is the only part of the code that would need to be changed in case a species is to be added. To keep the code tidy the packages {tidyverse} (Hadley Wickham et al. 2019) and {here} (Müller n.d.) are used. {BiocManager} (Ramos and Morgan n.d.) was also used to install packages. 8.4 Generating the first tree 8.4.1 Obtaining FASTA sequences The file is read by R and filtered so that the comments (species’ names) are removed, and only the GenBank identifier remains. Then, using the package {rentrez} (Winter 2017), the FASTA sequences are obtained and stored in a vector. The FASTA sequences are then stored in a file so they can be used as input for the {ape} function read.FASTA, where the sequences will be converted to a vector of the class DNAbin. species &lt;-scan(file = here(&quot;data/species.txt&quot;), what = &quot;character&quot;, sep = &quot;\\n&quot;, comment.char = &quot;#&quot;) #import list of species to be analyzed fasta_seqs &lt;- entrez_fetch(db = &quot;nucleotide&quot;, id = species, rettype = &quot;fasta&quot;) #retrieve the fasta sequences of the mitochondrial genome of every species in the list write(fasta_seqs, file=&quot;temp&quot;) grep(&quot;^*$&quot;, readLines(&quot;temp&quot;), invert = TRUE, value = TRUE) %&gt;% write(file = here(&quot;data/sequences.fasta&quot;)) #remove empty lines so read.FASTA works file.remove(&quot;temp&quot;) dna &lt;- read.FASTA(here(&quot;data/sequences.fasta&quot;), type = &quot;DNA&quot;) #store the sequences in a vector of the type &quot;DNAbin&quot; 8.4.2 Performing multiple sequence alignment Now that the dna has been stored as DNAbin, a multiple sequence alignment will be performed using Clustal Omega 1.2.3 (Sievers et al. 2011). The aligned DNA is saved as a file, so the alignment only has to be performed once (since it takes a while). dna_aligned &lt;- clustalomega(dna, exec = here(&quot;clustalo&quot;)) #use Clustal Omega 1.2.3 to perform multiple sequence alignment saveRDS(dna_aligned, here(&quot;data/dna_aligned.RDS&quot;)) #save as file so the code only has to run once 8.4.3 Importing the aligned dna dna_aligned &lt;- readRDS(here(&quot;data/dna_aligned.RDS&quot;)) #import the aligned dna 8.4.4 Calculating the genetic distances between the sequences and drawing a tree The different dna sequences have been aligned and now the genetic distances between the sequences can be calculated using the Tamura and Nei 1993 model (TN93) (Tamura and Nei 1993). The distance matrix can be used as input to calculate the phylogenetic tree. The used algorithm is the BIONJ algorithm of Gascuel (Gascuel 1997). The generated tree is visualized as a cladogram using the package {ggtree} (Yu et al. 2017). It’s visualized as a cladogram for now to keep it easy to read while it’s being improved. dna_dist &lt;- dist.dna(dna_aligned, model = &quot;TN93&quot;) #calculate genetic distances and store in a vector of the type &quot;dist&quot; treebionj &lt;- bionj(dna_dist) #create a tree of the class &quot;phylo&quot; using the method neighbor joining ggtree(treebionj, branch.length = &quot;none&quot;, ladderize = F)+ #visualize the tree as a cladogram theme_tree()+ geom_tiplab(size = 6, as_ylab = T)+ labs(title = &quot;Unrooted neighbor joining cladogram of\\nmitochondrial genomes of 15 primates and the golden hamster&quot;) Figure 8.1: Unrooted neighbor joining cladogram of mitochondrial genomes of 15 primates and the golden hamster. 8.5 Different types of phylogenetic models This first tree 8.1 was generated using an improved neighbor joining model, which is just one of the four most commonly used (Yoshida and Nei 2016). Neighbor joining only looks at the distance between the sequences, but not at smaller details in the sequences (Yoshida and Nei 2016). A different method is maximum likelihood, where the likelihood of the different possible states of the tree is calculated, and the tree with the highest likelihood is generated (Dhar and Minin 2016). Two other popular methods are Bayesian inference and maximum parsimony (Dhar and Minin 2016); (Yoshida and Nei 2016). Since maximum likelihood is the preferred method in publications (Dhar and Minin 2016), that is the method I will be using. 8.5.1 Finding the best model The input for the {phangorn} modelTest function is an object of the class phyDat, so first the DNAbin object has to be converted: dna_phydat &lt;- as.phyDat(dna_aligned) #convert object to class phyDat The model test is ran and the model with the lowest Akaike information criterion (AIC, calculated by the {phangorn} function modelTest), making it the best model (Cavanaugh and Neath 2019), is saved, again so this only has to be performed once as it takes quite long. modeltest &lt;- modelTest(dna_phydat) #run the model test optmodel &lt;- modeltest$Model[modeltest$AIC==min(modeltest$AIC)] #find the model with the lowest AIC saveRDS(optmodel, here(&quot;data/optmodel&quot;)) #save as file so the code only has to run once optmodel &lt;- readRDS(here(&quot;data/optmodel&quot;)) #import the data again 8.5.2 Generating a maximum likelihood tree Using the neighbor joining tree, the aligned dna and the best model the maximum likelihood tree can be computed: mlparsed &lt;- pml(treebionj, dna_phydat) #generate the maximum likelihood tree The optim.pml function from {phangorn} is used to optimize the different model parameters. The model found using modelTest is not an option in the optim.pml function, so the closest option is used. optmodel &lt;- gsub(&quot;\\\\+.*&quot;,&quot;&quot;, optmodel) #change the string so the optim.pml function can read it mlparsedoptim &lt;- optim.pml(mlparsed, optNni=TRUE, model = optmodel) #optimize the tree 8.5.3 Visualizing the maximum likelihood cladogram treeml &lt;- ladderize(mlparsedoptim$tree) #ladderize the tree ggtree(treeml, branch.length = &quot;none&quot;, ladderize = F)+ #visualize the tree as a cladogram theme_tree()+ geom_tiplab(size = 6, as_ylab = T)+ labs(title = &quot;Unrooted maximum likelihood cladogram of\\nmitochondrial genomes of 15 primates and the golden hamster&quot;) Figure 8.2: Unrooted maximum likelihood cladogram of mitochondrial genomes of 15 primates and the golden hamster 8.5.4 Rooting the tree The last step is to root the tree and clean up the tip labels (species names). It is important to root the tree, trees that aren’t rooted correctly may be misleading (Kinene et al. 2016). There are multiple ways to root a tree, I will be using an “outgroup”, a species that is genetically very different from the other species in the tree (Kinene et al. 2016). rootedml &lt;- root(treeml, outgroup = &quot;NC_013276.1 Mesocricetus auratus mitochondrion, complete genome&quot;, resolve.root = TRUE) #rooting the tree rootedml$tip.label &lt;- gsub(&quot;.*\\\\.[12]&quot;, &quot;&quot;, rootedml$tip.label) #clean up the labels rootedml$tip.label &lt;- gsub(&quot;(mitochondrion, | isolate | voucher).*genome&quot;, &quot;&quot;, rootedml$tip.label) #clean up the labels ggtree(rootedml, ladderize = F)+ #visualize the tree as a phylogenetic tree theme_tree()+ geom_tiplab(size = 2.5, as_ylab = F)+ xlim(0, 0.35)+ labs(title = &quot;Rooted maximum likelihood phylogenetic tree of\\nmitochondrial genomes of 15 primates and the golden hamster&quot;) Figure 8.3: Rooted maximum likelihood phylogenetic tree of mitochondrial genomes of 15 primates and the golden hamster The final tree 8.3 is visualized as a phylogenetic tree instead of a cladogram, with different branch lengths describing the ancestry of the primates. The cladogram only showed how the different species are related (Pearson et al. 2013). Normally, more thought would go into selecting the best possible outgroup (Rota-Stabelli and Telford 2008). Since my goal was to learn how to generate a phylogenetic tree in R (within a reasonable timeframe) I chose an animal that I like and know is more genetically different from the primates than the primates between each other. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
