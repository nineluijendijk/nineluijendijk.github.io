[["index.html", "Portfolio 1 Prerequisites", " Portfolio Nine Luijendijk 2023-01-01 1 Prerequisites Work in progress "],["resume.html", "2 Curriculum Vitae Nine Luijendijk 2.1 Contact 2.2 Education 2.3 Experience 2.4 Skills 2.5 Languages", " 2 Curriculum Vitae Nine Luijendijk 2.1 Contact E-mail: nineluijendijk@gmail.com Phone: +31 6 17210407 LinkedIn: https://www.linkedin.com/in/nine-luijendijk-75a973257/\\ GitHub: https://github.com/nineluijendijk 2.2 Education Hogeschool Utrecht Bachelor of Science - BS, Life Sciences, Sep. 2020 - Aug. 2024 Specialized in Data Sciences for Biology GPA of 4.0 2.3 Experience Zeeman textielSupers - Leusden Sales Associate, Nov. 2021 - Present StudyWorks BV - Leusden Tutor, Jan. 2020 - Sep. 2021 Tutored high school students in English, math and chemistry. Albert Heijn - Leusden Sales Associate, Jan. 2020 - Jul. 2020 2.4 Skills R programming language Bash command language 2.5 Languages Dutch English "],["guerillaframework.html", "3 Guerilla Analytics framework", " 3 Guerilla Analytics framework To keep my data manageable I use the Guerilla Analytics framework. This means I make sure every project has its own folder, every folder that needs it has a README and no raw data file is altered. The way I manage my data is visible in the tree below: dir_tree(&quot;/Users/nineluijendijk/Desktop/daur2&quot;) knitr::include_graphics(here(&quot;data/screenshot_dirtree.png&quot;)) "],["datanalysis.html", "4 Analyzing data from an experiment 4.1 Open the file in R and inspect the data. 4.2 Fixing the character/double issue 4.3 Think about how you would analyze this experiment to learn whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve (IC50). Write down you analysis as a step-wise plan. 4.4 Normalize the data for the controlNegative in such a way that the mean value for controlNegative is exactly equal to 1 and that all other values are expressed as a fraction thereof. Rerun your graphs with the normalized data.", " 4 Analyzing data from an experiment In this experiment, the number of offspring of adult C. elegans was counted after exposing the nematodes to different substances in multiple concentrations. The data was supplied by J. Louter of the INT/ILC. 4.1 Open the file in R and inspect the data. celegans_data &lt;- read_excel(&quot;/Users/nineluijendijk/Downloads/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) Expected data types would be double/integer for RawData (number of offspring), character for compName (name of the compound) and double for compConcentration (concentration of the compound). The actual data types of the columns are double for RawData, character for compName and character for compConcentration. The data type for compConcentration has not been correctly assigned which will cause the following issue if it’s not changed: #Create a scatter plot plot_chr &lt;- ggplot(data = celegans_data, aes(x = compConcentration, y = RawData))+ geom_point(aes(color = compName, shape = expType), size = 1)+ rotate_axis_labels(&quot;x&quot;, 90)+ labs(x = &quot;Concentration&quot;, y = &quot;Number of offspring&quot;, title = &quot;Number of C. elegans offspring under\\na number of circumstances, alphabetical&quot;, shape = &quot;Type&quot;, color = &quot;Compound&quot;)+ scale_colour_manual(values = c(&quot;red&quot;, &quot;darkgoldenrod1&quot;, &quot;green&quot;, &quot;royalblue3&quot;, &quot;violet&quot;))+ theme(legend.key.size = unit(0.75,&quot;line&quot;), legend.text = element_text(size = 8)) plot_chr (#fig:alphabetical_plot)Number of C. elegans offspring under a number of circumstances where the x-axis is ordered alphabetically The x-axis labels are ordered alphabetically because the data type of the compConcentration is character instead of double. R probably read it this way because Excel adds an E to showcase exponential values. 4.2 Fixing the character/double issue After changing the data type from character to double and adding jitter to the plot it looks as follows: #Change data type from chr to dbl celegans_data$compConcentration &lt;- as.double(celegans_data$compConcentration) #Create a scatter plot for the concentration in nM celegans_data_nM &lt;- celegans_data %&gt;% filter(compUnit == &quot;nM&quot;) plot_nM &lt;- ggplot(data = celegans_data_nM, aes(x = log10(compConcentration), y = RawData))+ geom_jitter(aes(color = compName, #add jitter shape = expType), width = 0.5, height = 0.2)+ labs(x = &quot;log10 concentration (nM)&quot;, y = &quot;Number of offspring&quot;)+ coord_cartesian(ylim = c(0, 120))+ scale_shape_manual(values = 3)+ scale_colour_manual(values = c(&quot;red&quot;, &quot;darkgoldenrod1&quot;, &quot;royalblue3&quot;))+ theme(legend.position = &quot;none&quot;) #Create a scatter plot for the concentration in pct celegans_data_pct &lt;- celegans_data %&gt;% filter(compUnit == &quot;pct&quot;) plot_pct &lt;- ggplot(data = celegans_data_pct, aes(x = expType, y = RawData))+ geom_jitter(aes(color = compName, #add jitter shape = expType), size = 1.3, width = 0.1, height = 0.075)+ xlab(&quot;Type&quot;)+ coord_cartesian(ylim = c(23.7, 121))+ scale_colour_manual(values = c(&quot;green&quot;, &quot;violet&quot;))+ theme(legend.position = &quot;none&quot;, axis.text.x=element_text(vjust=0.5, hjust=0.5, size = 8.75), axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank())+ #remove any y-axis labeling rotate_axis_labels(&quot;x&quot;, 90) #obtain legend legend &lt;- get_legend(plot_chr) #combine figures and legend for readability ggdraw(plot_grid(plot_grid(plot_nM, plot_pct), plot_grid(NULL, legend, ncol = 3), rel_widths = c(1, 0.35)))+ plot_annotation(&quot;Number of C. elegans offspring under\\na number of circumstances&quot;) The positive control for this experiment is ethanol. The negative control for this experiment is S-medium. 4.3 Think about how you would analyze this experiment to learn whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve (IC50). Write down you analysis as a step-wise plan. 4.4 Normalize the data for the controlNegative in such a way that the mean value for controlNegative is exactly equal to 1 and that all other values are expressed as a fraction thereof. Rerun your graphs with the normalized data. #Obtain the mean of the RawData (negativeControl) controlNeg &lt;- celegans_data %&gt;% filter(compName == &quot;S-medium&quot;) %&gt;% summarize(mean = mean(RawData, na.rm = TRUE)) #Use the mean to calculate fractions mutated &lt;- celegans_data %&gt;% filter(RawData &gt; 0) %&gt;% select(RawData, compName, compConcentration, expType) %&gt;% na.omit() %&gt;% mutate(normalized = RawData/controlNeg$mean) #Obtain the means of the other 2 control groups controlPos &lt;- mutated %&gt;% filter(expType == &quot;controlPositive&quot;) %&gt;% summarize(mean = mean(normalized, na.rm = TRUE)) controlVeh &lt;- mutated %&gt;% filter(expType == &quot;controlVehicleA&quot;) %&gt;% summarize(mean = mean(normalized, na.rm = TRUE)) mutated %&gt;% filter(compName == &quot;2,6-diisopropylnaphthalene&quot; | compName == &quot;decane&quot; | compName == &quot;naphthalene&quot;) %&gt;% ggplot(aes(x = log10(compConcentration), y = normalized))+ geom_jitter(aes(color = compName), width = 0.5, height = 0.1)+ scale_colour_manual(values = c(&quot;red&quot;, &quot;darkgoldenrod1&quot;, &quot;royalblue3&quot;))+ coord_cartesian(ylim = c(0, 1.5))+ labs(x = &quot;log10 concentration (nM)&quot;, y = &quot;Normalized number of offspring&quot;, title = &quot;Number of C. elegans offspring as a fraction\\nof the negative control group&quot;, color = &quot;Compound&quot;)+ geom_hline(yintercept = 1, color = &quot;violet&quot;)+ geom_hline(yintercept = controlPos$mean, color = &quot;green&quot;)+ geom_hline(yintercept = controlVeh$mean, color = &quot;green&quot;, linetype = &quot;longdash&quot;)+ guides(color = &quot;none&quot;)+ annotate(&quot;text&quot;, x = -3.8, y = 0.55, label = &quot;controlPositive&quot;, size = 2)+ annotate(&quot;text&quot;, x = -3.8, y = 0.95, label = &quot;controlNegative&quot;, size = 2)+ annotate(&quot;text&quot;, x = -3.8, y = 1.05, label = &quot;controlVehicleA&quot;, size = 2)+ facet_wrap(~ compName) This way it’s easier to read the graph. Everything below the pink line means less offspring than control C. elegans and everything above it means more. "],["primates.html", "5 Generating a phylogenetic tree in R 5.1 The plan 5.2 Preparatory research 5.3 Getting started 5.4 Generating the first tree 5.5 Different types of phylogenetic models", " 5 Generating a phylogenetic tree in R 5.1 The plan In 2 years’ time I want to be doing my master’s in Biology at Universiteit Leiden. I would like to specialize in Evolutionary Biology. Next semester I will be attending classes at Radboud Universiteit that are part of their minor Biology - Adaptive Organisms to get a head start on learning about evolutionary biology, both because it’s something I enjoy and to make the transition from life sciences to the master’s program easier I would like to learn more about the differences between multiple primates and generate a phylogenetic tree. Prior to me starting on the actual code for this phylogenetic tree, I had to look into the different types of trees and how to generate them. 5.2 Preparatory research I started by looking for R packages having to do with phylogenetic research. There were ample packages to help generate phylogenetic trees, but none of them seemed to have all the tools needed. Wanting to keep my research as reproducible as possible, my main priority was to keep every single part of the research in R, which is free and open source software (R Core Team 2014). To get an idea of how to get started, I read a couple of papers by researchers who’ve created phylogenetic trees. In these papers, most of the software used to generate the trees (and all the steps that come before) was not R (Pozzi et al. 2014); (Perelman et al. 2011). Some of the programs were point and click as well (Suchard et al. 2018); (Vaidya, Lohman, and Meier 2011), making the research less reproducible. After looking at some different R packages I decided {ape} (Paradis and Schliep 2019) was the best option, seeing as most packages for phylogenetic research are dependent on it (Jombart and Ahmed 2011). The first step in the analysis was to obtain the data from an online database. NCBI has their own command-line tool called NCBI Datasets (NCBI Staff 2022). The command-line tool was installed using Miniconda3 (Anaconda n.d.). The tool did work fine, but it wasn’t exactly what I was looking for. This was also when it was decided to perform the analysis on the mitochondrial genomes of the primates, instead of the nuclear genomes, due to computational limitations. 5.3 Getting started I ended up making a list of the species of interest, containing their GenBank (Benson et al. 2013) identifier, scientific name and their common name, and saving it in a text file called species.txt. This is the only part of the code that would need to be changed in case a species is to be added. To keep the code tidy the packages {tidyverse} (Hadley Wickham et al. 2019) and {here} (Müller n.d.) are used. {BiocManager} (Ramos and Morgan n.d.) was also used to install packages. 5.4 Generating the first tree 5.4.1 Obtaining FASTA sequences The file is read by R and filtered so that the comments (species’ names) are removed, and only the GenBank identifier remains. Then, using the package {rentrez} (Winter 2017), the FASTA sequences are obtained and stored in a vector. The FASTA sequences are then stored in a file so they can be used as input for the {ape} function read.FASTA, where the sequences will be converted to a vector of the class DNAbin. species &lt;-scan(file = here(&quot;data/species.txt&quot;), what = &quot;character&quot;, sep = &quot;\\n&quot;, comment.char = &quot;#&quot;) #import list of species to be analyzed fasta_seqs &lt;- entrez_fetch(db = &quot;nucleotide&quot;, id = species, rettype = &quot;fasta&quot;) #retrieve the fasta sequences of the mitochondrial genome of every species in the list write(fasta_seqs, file=&quot;temp&quot;) grep(&quot;^*$&quot;, readLines(&quot;temp&quot;), invert = TRUE, value = TRUE) %&gt;% write(file = here(&quot;data/sequences.fasta&quot;)) #remove empty lines so read.FASTA works file.remove(&quot;temp&quot;) dna &lt;- read.FASTA(here(&quot;data/sequences.fasta&quot;), type = &quot;DNA&quot;) #store the sequences in a vector of the type &quot;DNAbin&quot; 5.4.2 Performing multiple sequence alignment Now that the dna has been stored as DNAbin, a multiple sequence alignment will be performed using Clustal Omega 1.2.3 (Sievers et al. 2011). The aligned DNA is saved as a file, so the alignment only has to be performed once (since it takes a while). dna_aligned &lt;- clustalomega(dna, exec = here(&quot;clustalo&quot;)) #use Clustal Omega 1.2.3 to perform multiple sequence alignment saveRDS(dna_aligned, here(&quot;data/dna_aligned.RDS&quot;)) #save as file so the code only has to run once 5.4.3 Importing the aligned dna dna_aligned &lt;- readRDS(here(&quot;data/dna_aligned.RDS&quot;)) #import the aligned dna 5.4.4 Calculating the genetic distances between the sequences and drawing a tree The different dna sequences have been aligned and now the genetic distances between the sequences can be calculated using the Tamura and Nei 1993 model (TN93) (Tamura and Nei 1993). The distance matrix can be used as input to calculate the phylogenetic tree. The used algorithm is the BIONJ algorithm of Gascuel (Gascuel 1997). The generated tree is visualized as a cladogram using the package {ggtree} (Yu et al. 2017). It’s visualized as a cladogram for now to keep it easy to read while it’s being improved. dna_dist &lt;- dist.dna(dna_aligned, model = &quot;TN93&quot;) #calculate genetic distances and store in a vector of the type &quot;dist&quot; treebionj &lt;- bionj(dna_dist) #create a tree of the class &quot;phylo&quot; using the method neighbor joining ggtree(treebionj, branch.length = &quot;none&quot;, ladderize = F)+ #visualize the tree as a cladogram theme_tree()+ geom_tiplab(size = 6, as_ylab = T)+ labs(title = &quot;Unrooted neighbor joining cladogram of\\nmitochondrial genomes of 15 primates and the golden hamster&quot;) 5.5 Different types of phylogenetic models This first tree was generated using an improved neighbor joining model, which is just one of the four most commonly used (Yoshida and Nei 2016). Neighbor joining only looks at the distance between the sequences, but not at smaller details in the sequences (Yoshida and Nei 2016). A different method is maximum likelihood, where the likelihood of the different possible states of the tree is calculated, and the tree with the highest likelihood is generated (Dhar and Minin 2016). Two other popular methods are Bayesian inference and maximum parsimony (Dhar and Minin 2016); (Yoshida and Nei 2016). Since maximum likelihood is the preferred method in publications (Dhar and Minin 2016), that is the method I will be using. 5.5.1 Finding the best model The input for the {phangorn} modelTest function is an object of the class phyDat, so first the DNAbin object has to be converted: dna_phydat &lt;- as.phyDat(dna_aligned) #convert object to class phyDat The model test is ran and the model with the lowest Akaike information criterion (AIC, calculated by the {phangorn} function modelTest), making it the best model (Cavanaugh and Neath 2019), is saved, again so this only has to be performed once as it takes quite long. modeltest &lt;- modelTest(dna_phydat) #run the model test optmodel &lt;- modeltest$Model[modeltest$AIC==min(modeltest$AIC)] #find the model with the lowest AIC saveRDS(optmodel, here(&quot;data/optmodel&quot;)) #save as file so the code only has to run once optmodel &lt;- readRDS(here(&quot;data/optmodel&quot;)) #import the data again 5.5.2 Generating a maximum likelihood tree Using the neighbor joining tree, the aligned dna and the best model the maximum likelihood tree can be computed: mlparsed &lt;- pml(treebionj, dna_phydat) #generate the maximum likelihood tree The optim.pml function from {phangorn} is used to optimize the different model parameters. The model found using modelTest is not an option in the optim.pml function, so the closest option is used. optmodel &lt;- gsub(&quot;\\\\+.*&quot;,&quot;&quot;, optmodel) #change the string so the optim.pml function can read it mlparsedoptim &lt;- optim.pml(mlparsed, optNni=TRUE, model = optmodel) #optimize the tree 5.5.3 Visualizing the maximum likelihood cladogram treeml &lt;- ladderize(mlparsedoptim$tree) #ladderize the tree ggtree(treeml, branch.length = &quot;none&quot;, ladderize = F)+ #visualize the tree as a cladogram theme_tree()+ geom_tiplab(size = 6, as_ylab = T)+ labs(title = &quot;Unrooted maximum likelihood cladogram of\\nmitochondrial genomes of 15 primates and the golden hamster&quot;) 5.5.4 Rooting the tree The last step is to root the tree and clean up the tip labels (species names). It is important to root the tree, trees that aren’t rooted correctly may be misleading (Kinene et al. 2016). There are multiple ways to root a tree, I will be using an “outgroup”, a species that is genetically very different from the other species in the tree (Kinene et al. 2016). rootedml &lt;- root(treeml, outgroup = &quot;NC_013276.1 Mesocricetus auratus mitochondrion, complete genome&quot;, resolve.root = TRUE) #rooting the tree rootedml$tip.label &lt;- gsub(&quot;.*\\\\.[12]&quot;, &quot;&quot;, rootedml$tip.label) #clean up the labels rootedml$tip.label &lt;- gsub(&quot;(mitochondrion, | isolate | voucher).*genome&quot;, &quot;&quot;, rootedml$tip.label) #clean up the labels ggtree(rootedml, ladderize = F)+ #visualize the tree as a phylogenetic tree theme_tree()+ geom_tiplab(size = 2.5, as_ylab = F)+ xlim(0, 0.35)+ labs(title = &quot;Rooted maximum likelihood phylogenetic tree of\\nmitochondrial genomes of 15 primates and the golden hamster&quot;) The final tree is visualized as a phylogenetic tree instead of a cladogram, with different branch lengths describing the ancestry of the primates. The cladogram only showed how the different species are related (Pearson et al. 2013). Normally, more thought would go into selecting the best possible outgroup (Rota-Stabelli and Telford 2008). Since my goal was to learn how to generate a phylogenetic tree in R (within a reasonable timeframe) I chose an animal that I like and know is more genetically different from the primates than the primates between each other. References "],["working-with-relational-databases.html", "6 Working with relational databases 6.1 Preparing the data 6.2 Inspecting the data 6.3 Combining the dataframes 6.4 Visualizing the data", " 6 Working with relational databases Having spent so much time analyzing data, it would be useful to learn more about obtaining data from databases, and how to manipulate and analyze relational data using SQL. 6.1 Preparing the data 6.1.1 Introduction In this report I will be analyzing 2 datasets obtained from Google Dengue Trends (http://www.google.org/denguetrends) and Google Flu Trends (http://www.google.org/flutrends). The datasets contain the number of dengue and flu cases per week of the year, from 2002 through 2015. 6.1.2 Importing the data and making it tidy The first step in the data analysis is to import the data into Rstudio and make it tidy, meaning each variable gets its own column, each observation gets its own row and each value its own cell. In this case neither of the datasets were in tidy format, as there were multiple observations in each row. After making the data tidy, the class of every column was checked and changed to match the class of that column in the gapminder ({dslabs}, (Irizarry and Gill 2021)) dataframe, so they can later be joined together. A “year” column was added to both the dengue and the flu data as well, to match the “year” column of gapminder. dengue_data &lt;- read_csv(here(&quot;data_raw/dengue_data.csv&quot;), skip = 11) #first 11 rows contain metadata flu_data &lt;- read_csv(here(&quot;data_raw/flu_data.csv&quot;), skip = 11) #first 11 rows contain metadata dengue_tidy &lt;- pivot_longer(data = dengue_data, cols = c(2:ncol(dengue_data)), names_to = &quot;country&quot;, values_to = &quot;cases&quot;) #each observation must have its own row flu_tidy &lt;- pivot_longer(data = flu_data, cols = c(2:ncol(flu_data)), names_to = &quot;country&quot;, values_to = &quot;cases&quot;) #each observation must have its own row dengue_tidy &lt;- dengue_tidy %&gt;% mutate(&quot;year&quot; = substr(Date, 1, 4)) #add &quot;year&quot; column dengue_tidy$year &lt;- as.integer(dengue_tidy$year) #change class of column year from chr to int flu_tidy &lt;- flu_tidy %&gt;% mutate(&quot;year&quot; = substr(Date, 1, 4)) #add &quot;year&quot; column flu_tidy$year &lt;- as.integer(flu_tidy$year) #change class of column year from chr to int dengue_tidy$country &lt;- as.factor(dengue_tidy$country) #the column country in gapminder is of the class factor flu_tidy$country &lt;- as.factor(flu_tidy$country) #the column country in gapminder is of the class factor The tidy dataframes were then saved as separate rds and csv files: saveRDS(gapminder, here(&quot;data/gapminder.rds&quot;)) #store the three tables as .rds and .csv files saveRDS(dengue_tidy, here(&quot;data/dengue.rds&quot;)) saveRDS(flu_tidy, here(&quot;data/flu.rds&quot;)) write_csv(gapminder, here(&quot;data/gapminder.csv&quot;)) write_csv(dengue_tidy, here(&quot;data/dengue.csv&quot;)) write_csv(flu_tidy, here(&quot;data/flu.csv&quot;)) 6.1.3 Connecting to the database For this analysis, a PostgreSQL database was created in Dbeaver. To connect to the database in Rstudio, the {RPostgres} (H. Wickham, Ooms, and Müller 2022) package was used. con &lt;- dbConnect(Postgres(), #connect to the database dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;password&quot;) 6.1.4 Exporting the tidy dataframes to the database dbWriteTable(con, &quot;gapminder&quot;, gapminder) #insert the tables into the database dbWriteTable(con, &quot;dengue&quot;, dengue_tidy) dbWriteTable(con, &quot;flu&quot;, flu_tidy) 6.2 Inspecting the data 6.2.1 Inspecting the data using SQL To make sure no important data is missing, the tables were checked for NULL values. Data I consider important is the year and the country data, since if there are any missing values here errors may occur. Checking for NULL values SELECT * FROM gapminder WHERE year IS NULL; --make sure no important values are missing Table 6.1: 0 records country year infant_mortality life_expectancy fertility population gdp continent region SELECT * FROM gapminder WHERE country IS NULL; Table 6.2: 0 records country year infant_mortality life_expectancy fertility population gdp continent region SELECT * FROM dengue WHERE year IS NULL; Table 6.3: 0 records Date country cases year SELECT * FROM dengue WHERE country IS NULL; Table 6.4: 0 records Date country cases year SELECT * FROM flu WHERE year IS NULL; Table 6.5: 0 records Date country cases year SELECT * FROM flu WHERE country IS NULL; Table 6.6: 0 records Date country cases year There were no records for any of these selects, meaning there are no NULL values anywhere in “year” or “country”. Now to actually look at the data, there were a couple of statistics I was interested in, like: How many observations were done SELECT COUNT(cases) --count the number of observations (ignoring NULL) FROM dengue; Table 6.7: 1 records count 6263 SELECT COUNT(cases) FROM flu; Table 6.8: 1 records count 17266 The total number of cases SELECT SUM(cases) --calculate the total number of cases FROM dengue; Table 6.9: 1 records sum 870.376 SELECT SUM(cases) FROM flu; Table 6.10: 1 records sum 8179518 The countries with the highest number of cases SELECT year, country, cases*1000 AS cases1000 --look at the highest observed numbers of cases FROM dengue WHERE cases IS NOT NULL ORDER BY cases1000 DESC; Table 6.11: Displaying records 1 - 10 year country cases1000 2008 Brazil 1000 2004 Indonesia 1000 2012 India 1000 2009 Mexico 1000 2010 Philippines 1000 2009 Bolivia 1000 2013 Thailand 1000 2009 Argentina 1000 2010 Venezuela 1000 2005 Singapore 1000 SELECT year, country, cases FROM flu WHERE cases IS NOT NULL ORDER BY cases DESC; Table 6.12: Displaying records 1 - 10 year country cases 2013 United States 10555 2013 United States 10112 2009 Canada 9688 2009 Canada 9630 2013 United States 9408 2012 United States 8618 2003 United States 8196 2012 United States 7904 2013 Canada 7698 2003 Canada 7531 6.2.2 Inspecting the data using R Now it’s time to inspect the data in R. When importing the dengue data from its csv, I noticed every value in the dataframe was a fraction instead of a whole number. Since no Readme was provided with the data, I am not sure why this is. To make the values whole, I multiplied the number of cases by 1000. To find out which countries had the highest and lowest observed number of cases, the following code was used: dengue_mutated &lt;- dengue_tidy %&gt;% mutate(&quot;cases1000&quot; = cases * 1000) #add a column with the multiplied number of cases maxcasesdengue &lt;- subset(dengue_mutated, cases1000 == max(na.omit(dengue_mutated$cases1000))) mincasesdengue &lt;- subset(dengue_mutated, cases1000 == min(na.omit(dengue_mutated$cases1000))) unique(maxcasesdengue$country) #look at which countries have the highest observed number of dengue cases ## [1] Indonesia Singapore Brazil Bolivia Argentina Mexico ## [7] Venezuela Philippines India Thailand ## 10 Levels: Argentina Bolivia Brazil India Indonesia Mexico ... Venezuela unique(mincasesdengue$country) #look at which countries have the lowest observed number of dengue cases ## [1] India Bolivia ## 10 Levels: Argentina Bolivia Brazil India Indonesia Mexico ... Venezuela maxcasesflu &lt;- subset(flu_tidy, cases == max(na.omit(flu_tidy$cases))) mincasesflu &lt;- subset(flu_tidy, cases == min(na.omit(flu_tidy$cases))) unique(maxcasesflu$country) #look at which countries have the highest observed number of flu cases ## [1] United States ## 29 Levels: Argentina Australia Austria Belgium Bolivia Brazil ... Uruguay unique(mincasesflu$country) #look at which countries have the lowest observed number of flu cases ## [1] Chile Sweden ## 29 Levels: Argentina Australia Austria Belgium Bolivia Brazil ... Uruguay Another statistic is the total number and the average number of cases per country of all years combined. dengue_country_means &lt;- dengue_mutated %&gt;% group_by(country) %&gt;% summarize(total = sum(cases1000, na.rm = TRUE), mean = mean(cases1000, na.rm = TRUE)) %&gt;% arrange(desc(mean)) #calculate the total number of cases and mean number of cases per country flu_country_means &lt;- flu_tidy %&gt;% group_by(country) %&gt;% summarize(total = sum(cases, na.rm = TRUE), mean = mean(cases, na.rm = TRUE)) %&gt;% arrange(desc(mean)) Dengue numbers country total mean Venezuela 180893 277.44325 Thailand 66761 184.93352 Indonesia 97124 147.38088 Singapore 94747 143.77390 Brazil 93638 142.09105 Mexico 92989 141.53577 Philippines 88106 136.38700 India 76889 116.67527 Argentina 49779 76.34816 Bolivia 29450 44.68892 Flu numbers country total mean South Africa 1349388 2698.776000 Canada 1169673 1886.569355 United States 1142909 1843.401613 Mexico 703432 1134.567742 Austria 500332 806.987097 Germany 496073 800.117742 Romania 463130 709.234303 Bulgaria 331824 595.734291 Russia 262781 463.458554 Australia 219016 436.286853 Ukraine 204308 397.486381 Bolivia 192202 295.241167 Japan 160897 261.196429 Paraguay 135348 253.460674 Peru 144810 219.742033 Brazil 138825 210.660091 Argentina 100391 153.503058 Belgium 76869 123.982258 Uruguay 76395 117.893518 France 61625 99.395161 Hungary 50572 90.146168 Norway 40462 72.124777 Switzerland 43573 70.850407 Poland 30456 53.619718 Spain 31856 51.380645 New Zealand 19595 39.033864 Netherlands 22408 36.200323 Chile 7544 11.482496 Sweden 2824 5.548134 Though interesting, these numbers only represent the total number of cases, not taking into account the size of the difference in population between countries. That is where the gapminder dataframe comes into play, containing exactly the data needed to include the population in the calculations. 6.3 Combining the dataframes 6.3.1 Preparing the tidy dataframes To prepare the dataframes for the joining NA values were removed, and both the sum and the weekly mean of the number of cases were added. #calculate the number of cases per year and the average number of cases per week per country dengue_combine &lt;- na.omit(dengue_mutated) %&gt;% group_by(country, year) %&gt;% summarize(total_cases_dengue = sum(cases1000), mean_weekly_dengue = mean(cases1000)) flu_combine &lt;- na.omit(flu_tidy) %&gt;% group_by(country, year) %&gt;% summarize(total_cases_flu = sum(cases), mean_weekly_flu = mean(cases)) 6.3.2 Joining the dataframes together The new dataframes were then exported to the database, where they were joined using SQL: dbWriteTable(con, &quot;dengue_combine&quot;, dengue_combine) #insert the new tables into the database dbWriteTable(con, &quot;flu_combine&quot;, flu_combine) CREATE TABLE dengue_gapminder AS --combine dengue_data and gapminder SELECT gapminder.country, gapminder.year, gapminder.population, gapminder.continent, gapminder.region, dengue_combine.total_cases_dengue, dengue_combine.mean_weekly_dengue FROM gapminder LEFT JOIN dengue_combine ON gapminder.country = dengue_combine.country AND gapminder.year = dengue_combine.year; CREATE TABLE flu_gapminder AS --combine flu_data and gapminder SELECT gapminder.country, gapminder.year, gapminder.population, gapminder.continent, gapminder.region, flu_combine.total_cases_flu, flu_combine.mean_weekly_flu FROM gapminder LEFT JOIN flu_combine ON gapminder.country = flu_combine.country AND gapminder.year = flu_combine.year; CREATE TABLE flu_gapminder_dengue AS --combine all 3 tables SELECT flu_gapminder.country, flu_gapminder.year, flu_gapminder.population, flu_gapminder.continent, flu_gapminder.region, flu_gapminder.total_cases_flu, flu_gapminder.mean_weekly_flu, dengue_combine.total_cases_dengue, dengue_combine.mean_weekly_dengue FROM flu_gapminder INNER JOIN dengue_combine ON flu_gapminder.country = dengue_combine.country AND flu_gapminder.year = dengue_combine.year; 6.3.3 Loading the combined tables into R dengue_gapminder &lt;- dbReadTable(con, &quot;dengue_gapminder&quot;) #load the combined tables into R flu_gapminder &lt;- dbReadTable(con, &quot;flu_gapminder&quot;) flu_gapminder_dengue &lt;- dbReadTable(con, &quot;flu_gapminder_dengue&quot;) dengue_gapminder_nona &lt;- na.omit(dengue_gapminder) #remove NA values flu_gapminder_nona &lt;- na.omit(flu_gapminder) flu_gapminder_dengue_nona &lt;- na.omit(flu_gapminder_dengue) 6.4 Visualizing the data 6.4.1 Normalizing the data Now that the dataframes are combined, the gapminder column “population” can be used to normalize the data. We now know the average number of weekly cases per 100.000 population, of every country per year. #normalize the data by calculating the average number of cases per 100,000 population dengue_gapminder_normalized &lt;- dengue_gapminder_nona %&gt;% mutate(&quot;normalized_dengue&quot; = (total_cases_dengue/population)*100000, &quot;normalized_dengue_mean&quot; = (mean_weekly_dengue/population)*100000) flu_gapminder_normalized &lt;- flu_gapminder_nona %&gt;% mutate(&quot;normalized_flu&quot; = (total_cases_flu/population)*100000, &quot;normalized_flu_mean&quot; = (mean_weekly_flu/population)*100000) flu_gapminder_dengue_normalized &lt;- flu_gapminder_dengue_nona %&gt;% mutate(&quot;normalized_dengue&quot; = (total_cases_dengue/population)*100000, &quot;normalized_flu&quot; = (total_cases_flu/population)*100000, &quot;normalized_dengue_mean&quot; = (mean_weekly_dengue/population)*100000, &quot;normalized_flu_mean&quot; = (mean_weekly_flu/population)*100000) 6.4.2 Calculating descriptive statistics In an attempt to find some descriptive statistics, I tried calculating the means, standard deviations and Shapiro-Wilk p-values (to see if the data is from a normal distribution). #calculate means, standard deviations and Shapiro-Wilk p-values per region dengue_gap_summary &lt;- dengue_gapminder_normalized %&gt;% group_by(region) %&gt;% summarize(mean = mean(total_cases_dengue), sd = sd(total_cases_dengue), pvalue_sw = shapiro.test(total_cases_dengue)$p.value) flu_gap_summary &lt;-flu_gapminder_normalized %&gt;% group_by(region) %&gt;% summarize(mean = mean(total_cases_flu), sd = sd(total_cases_flu), pvalue_sw = shapiro.test(total_cases_flu)$p.value) #calculate means, standard deviations and Shapiro-Wilk p-values per country dengue_gap_summary_country &lt;- dengue_gapminder_normalized %&gt;% group_by(country) %&gt;% summarize(mean = mean(total_cases_dengue), sd = sd(total_cases_dengue), pvalue_sw = shapiro.test(total_cases_dengue)$p.value) flu_gap_summary_country &lt;-flu_gapminder_normalized %&gt;% group_by(country) %&gt;% summarize(mean = mean(total_cases_flu), sd = sd(total_cases_flu), pvalue_sw = shapiro.test(total_cases_flu)$p.value) Not every country has a normal distribution (normal distribution is where p &gt; 0.05). After finding the countries that do, levene’s test was performed to check for equal variances: #find the countries where the data is a normal distribution countries_normal_dengue &lt;- subset(dengue_gap_summary_country, pvalue_sw &gt; 0.05)[,1] %&gt;% pull() countries_normal_flu &lt;- subset(flu_gap_summary_country, pvalue_sw &gt; 0.05)[,1] %&gt;% pull() #filter for the rows with observations from those countries dengue_normal &lt;- dengue_gapminder_normalized[dengue_gapminder_normalized$country %in% countries_normal_dengue,] flu_normal &lt;- flu_gapminder_normalized[flu_gapminder_normalized$country %in% countries_normal_flu,] #perform levene&#39;s test leveneTest(total_cases_dengue ~ country, data = dengue_normal, center = mean)[1,3] ## [1] 0.1828471 leveneTest(total_cases_flu ~ country, data = flu_normal, center = mean)[1,3] ## [1] 5.411843e-33 6.4.3 Visualizing correlation After calculating these p-values, I realized I’m more interested in correlation than difference between the groups, since it’s already known the groups are differt from each other (different countries). Instead of performing an ANOVA or Kruskal-Wallis test, it was decided that the pearson’s correlation coefficient would be calculated. A scatter plot was generated to visualize correlation: #perform a pearson correlation analysis dengue_flu_cor &lt;- round(cor.test(flu_gapminder_dengue_normalized$total_cases_dengue, flu_gapminder_dengue_normalized$total_cases_flu, method = &quot;pearson&quot;)$estimate, 3) cor_pvalue &lt;- cor.test(flu_gapminder_dengue_normalized$total_cases_dengue, flu_gapminder_dengue_normalized$total_cases_flu, method = &quot;pearson&quot;)$p.value #visualize the correlation in a scatter plot ggplot(data = flu_gapminder_dengue_normalized, aes(y = total_cases_dengue, x = total_cases_flu)) + geom_point()+ labs(title = &quot;Relation between dengue and flu cases&quot;, subtitle = paste(&quot;pearson&#39;s r = &quot;, dengue_flu_cor), y = &quot;Number of dengue cases&quot;, x = &quot;Number of flu cases&quot;)+ theme_minimal() There is a significant correlation between the number of dengue and flu cases in a country, as p is smaller than 0.05 at cor_pvalue. The correlation coefficient of dengue_flu_cor indicates a weak correlation (Taylor 1990). 6.4.4 Visualizing the dengue data Dengue is endemic in most tropical countries / regions (Gubler 2002), as shown below: #visualize the number of dengue cases per region in a bar plot ggplot(dengue_gap_summary, aes(x = region, y = mean, fill = region))+ geom_col()+ geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd, width = 0.2))+ labs(title = &quot;Number of dengue cases per region&quot;, subtitle = &quot;Error bars depict 1 standard deviation&quot;, x=&quot;Region&quot;, y=&quot;Average number of dengue cases&quot;)+ theme_minimal()+ theme(legend.position = &quot;none&quot;) This graph was made by looking at the average number of cases per region per year, meaning it uses the total number of cases rather that the normalized ones that were calculated earlier. A more representative plot would use these numbers: #create a scatter plot of the dengue data dengue_plot &lt;- ggplot(dengue_gapminder_normalized, aes(x = year, y = mean_weekly_dengue, group = country, color = country))+ geom_line()+ geom_point()+ labs(title = &quot;Weekly dengue cases per country&quot;, y = &quot;Average number of dengue cases per week per 100,000 population&quot;, x = &quot;Year&quot;, color = &quot;Country&quot;)+ scale_x_continuous(breaks = seq(from = min(dengue_gapminder_normalized$year), to = max(dengue_gapminder_normalized$year), by = 1))+ theme_minimal() ggplotly(dengue_plot) #visualize the number of dengue cases per country in an interactive scatter plot 6.4.5 Visualizing the flu data The same graphs can be made using the flu data: #visualize the number of flu cases per region in a bar plot ggplot(flu_gap_summary, aes(x = region, y = mean, fill = region)) + geom_col()+ geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd, width = 0.2))+ labs(title = &quot;Number of flu cases per region&quot;, subtitle = &quot;Error bars depict 1 standard deviation&quot;, x=&quot;Region&quot;, y=&quot;Average number of flu cases&quot;)+ theme_minimal()+ theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) This graph has the same issue as before, using the total number of cases rather than the normalized data. Northern America and Southern Africa stand out with their high number of cases. It is true that Northern America has a high number of weekly flu cases, but seeing their large population this was to be expected. More interesting is Southern Africa, where the population is smaller but the number of cases is higher. Looking at some random observations: SELECT DISTINCT country, region FROM flu_gapminder WHERE region = &#39;Southern Africa&#39; AND total_cases_flu IS NOT NULL Table 6.13: 1 records country region South Africa Southern Africa South Africa is the only country in region Southern Africa with flu data. SELECT year, country, cases FROM flu WHERE country = &#39;South Africa&#39; AND cases IS NOT NULL ORDER BY RANDOM(); Table 6.14: Displaying records 1 - 10 year country cases 2011 South Africa 2232 2013 South Africa 2341 2014 South Africa 2639 2015 South Africa 2967 2009 South Africa 1917 2011 South Africa 4046 2008 South Africa 2080 2007 South Africa 1797 2006 South Africa 2343 2006 South Africa 2019 SELECT year, country, cases FROM flu WHERE cases IS NOT NULL ORDER BY RANDOM(); Table 6.15: Displaying records 1 - 10 year country cases 2007 Ukraine 1175 2012 Spain 206 2003 Germany 1055 2013 South Africa 2418 2013 Canada 690 2015 South Africa 2626 2008 Russia 224 2014 Paraguay 213 2014 Brazil 179 2004 Canada 2722 We can see that the number of weekly cases in South Africa is very high compared to other countries, causing it to stand out when looking at the bar graph. This will also be reflected in the average number of weekly cases: #create a scatter plot of the flu data flu_plot &lt;- ggplot(flu_gapminder_normalized, aes(x = year, y = mean_weekly_flu, group = country, color = country))+ geom_line()+ geom_point()+ labs(title = &quot;Weekly flu cases per country&quot;, y = &quot;Average number of flu cases per week per 100,000 population&quot;, x = &quot;Year&quot;, color = &quot;Country&quot;)+ scale_x_continuous(breaks = seq(from = min(flu_gapminder_normalized$year), to = max(flu_gapminder_normalized$year), by = 1))+ theme_minimal() ggplotly(flu_plot) #visualize the number of flu cases per country in an interactive scatter plot Northern American countries United States and Canada still stand out, having higher weekly averages than most countries, despite the numbers being normalized for this graph. South Africa does have the highest average number of flu cases per week per 100,000 population since data started being collected from there. Now that the analysis and visualization is done, Rstudio can be disconnected from the database. dbDisconnect(con) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
